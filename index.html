<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jianxiang Feng</title>
  
  <meta name="author" content="Jianxiang Feng">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="images/favicon.ico">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jianxiang Feng (冯健祥)</name>
              </p>
              <p>I am a final-year PhD student at <a href="https://www.tum.de/en">Technical University of Munich (TUM)</a> and the <a href="https://www.dlr.de/rm/en/desktopdefault.aspx/tabid-8017/">Institute of Robotic and Mechtronic</a> (RM), <a href="https://www.dlr.de/en/">German Aerospace Center</a> (DLR). I am advised by Prof. <a href="https://rmc.dlr.de/rm/de/staff/rudolph.triebel/"> Rudolph Triebel</a> (DLR&KIT) and affiliated with <a href="https://www.mu-ds.de/"> Munich School of Data Science</a>, where I worked with Prof. <a href="https://www.cs.cit.tum.de/en/daml/team/damlguennemann/">Stephan Günnemann</a>.
              </p>
              <p>
                My research interests reside in the intersection of robotics and machine learning with primary focus on the <strong>trustworthy</strong> and <strong>adaptable</strong> learning ability of a robot in an <strong>open-world</strong> environment. In particular, aiming to equip a robot with <strong>introspective capabilities</strong> i.e., reliable confidence estimates and an awareness of the internal state of the system scuh as limitation of its knowledge and skills, I am intereseted in levaraging <strong>probabilistic Machine Learning</strong> methods (e.g. Bayesian Neural Networks, Probabilistic Graphicial Models, flow-based Deep Generative Models) on <strong>1.</strong> how to provide introspection (Uncertainty Estimation) and <strong>2.</strong> exploit this capability for robot perception and planning (Out-of-Distribution Detection, Active Learning, Robotic Assembly Sequence Planning, Feasibility Learning and so on). I believe that this can lead to more explainable and ultimately safer autononous robot systems.
              </p>
              <div id="more-bio" style="display: None">
                <p>Jianxiang Feng received his bachelor degree in electronic engineering from Beijing University of Posts and Telecommunication (BUPT), China (2011-2015) and his master degree in electronic and information technology from Technical University of Munich (TUM), Germany (2016-2019). Since August 2019, he is a research scientist at the Institute of Robotics and Mechatronics (RM), the German Aerospace Center (DLR), Germany. His research interests reside in the intersection of robotics and machine learning.</p>
              </div>
              <p style="text-align:center">
                <a href="javascript:toggle_bio()">Formal Bio</a>&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://drive.google.com/file/d/1N8rOIZGXaqS1BMtjclU0cBXdBWGmuwSK/view?usp=sharing">CV</a>&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://github.com/JianxiangFENG">Github</a>&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://scholar.google.com/citations?user=b-5CscIAAAAJ&hl=en">Google Scholar</a> &nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://www.linkedin.com/in/jianxiangfeng/">LinkedIn</a>&nbsp;&nbsp;&nbsp;&nbsp;
                <!-- <a href="https://twitter.com/fengjianxiang?lang=en">Twitter</a>&nbsp;&nbsp;&nbsp;&nbsp;
		            <a href="https://www.instagram.com/luffy_fjx/">Instagram</a> -->
                <br><br>
                jianxiang.feng at tum dot de
                <br><br>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/profile-half.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              <div class="list-item highlight" data-category="highlight">
	      <p class="date">08.2023</p> &nbsp;&nbsp;&nbsp; A <a href="https://openreview.net/forum?id=BzjLaVvr955&referrer=%5Bthe%20profile%20of%20Jianxiang%20Feng%5D(%2Fprofile%3Fid%3D~Jianxiang_Feng1">paper</a> on Normalizing Flows for Out-of-Distribution Detection got accepted in CoRL2023!
              </div><div class="list-item highlight" data-category="highlight">
		      
              <p class="date">07.2023</p> &nbsp;&nbsp;&nbsp; A <a href="https://arxiv.org/pdf/2303.10135.pdf">paper</a> on Graph Neural Networs and Assembly Seqeunce Planning got accepted in IROS2023.
              </div><div class="list-item highlight" data-category="highlight">

                <div class="list-item highlight" data-category="highlight">
                <p class="date">06.2023</p> &nbsp;&nbsp;&nbsp; Our <a href=https://www.linkedin.com/feed/update/urn:li:activity:7091522148384550913/?origin=SHARED_BY_YOUR_NETWORK>aerial manipulation demo</a> for <a href="https://www.kuka.com/en-us/future-production/research-and-development/kuka-innovation-award/kuka-innovation-award-2023">kuka inovation award 2023</a> gained immense popularity in <a href="https://automatica-munich.com/en/">Automatica</a> 2023 (<a href="https://www.linkedin.com/posts/kukaglobal_team-jarvis-from-the-merlin-labodratory-of-activity-7080170709481574400-7guO?utm_source=share&utm_medium=member_desktop">press</a>).
                </div><div class="list-item highlight" data-category="highlight">

              <div class="list-item highlight" data-category="highlight">
              <p class="date">06.2023</p> &nbsp;&nbsp;&nbsp; A paper of <a href="https://sites.google.com/nvidia.com/industrial-assembly">workshop</a> on Robotics and AI: The Future of Industrial Assembly Tasks at RSS 2023 got accepted.
              </div><div class="list-item highlight" data-category="highlight">

	    <div class="list-item highlight" data-category="highlight">
		 <p class="date">03.2023</p> &nbsp;&nbsp;&nbsp; Won the 1st place in the discipline assist robot race of <a href="https://cybathlon.ethz.ch/de/teams/edan">Cybathlon Challenge</a> as part of EDAN team (<a href="https://www.dlr.de/rm/en/desktopdefault.aspx/tabid-3755/17612_read-81519/year-2023/">press</a>, <a href="https://www.youtube.com/watch?v=EoER_5vYZsU&t=3s&ab_channel=DLRRM">video</a>)
            </div>
            </p>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Video Highlights</heading>
            <!-- <p>
              <h4> &emsp;&emsp;&emsp;&emsp; <a href="https://cybathlon.ethz.ch/de/teams/edan">Cybathlon Challenges </a> (assistant robot race) 
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;  Bayesian Active Learning Demo on <a href="https://www.dlr.de/rm/en/desktopdefault.aspx/tabid-11670/20388_read-47709/">EDAN</a>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Bayesian Active Learning Demo on <a href="https://www.youtube.com/watch?v=ZoNNjQfUdJw&ab_channel=DLRRM">SAM</a></h4>
              </p> -->
            <p></p>
            <p></p>
            <div class="list-item highlight previews" data-category="highlight">
              <a href="https://www.youtube.com/watch?v=EoER_5vYZsU&t=3s&ab_channel=DLRRM"><iframe width="350" height="250" src="https://www.youtube.com/embed/EoER_5vYZsU?&autoplay=1&loop=1&playlist=EoER_5vYZsU&mute=1&start=100&showinfo=0" title="Cybathlon" frameborder="0" allow="accelerometer; allow='autoplay'; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
              </a>  
              <!-- <h4> CYBATHLON Challenges (assistant robot race)</h4> -->
              
              <!-- <h4> Bayesian Active Learning Demo on an assitive robot EDAN</h4> -->
              <a href="https://arxiv.org/pdf/2109.11547"><iframe width="350" height="250" src="https://www.youtube.com/embed/kfWNEBbvuSE?&autoplay=1&loop=1&playlist=kfWNEBbvuSE&mute=1&start=31&showinfo=0" title="Edan" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
	          </a> 
              
             <!--  <h4> Bayesian Active Learning Demo on an assitive robot EDAN</h4>
              <a href="https://arxiv.org/pdf/2109.11547"><video width="560" height="315" playsinline="" muted="" autoplay="" loop=""><source src="images/bal_edan.mp4" type="video/mp4"></video></a> -->
              
              <!-- <h4> Bayesian Active Learning Demo on aerial manipulation robnot SAM</h4> -->
              <a href="https://www.youtube.com/watch?v=JRnPIARW8xY&ab_channel=DLRRM"><iframe width="350" height="250" src="https://www.youtube.com/embed/JRnPIARW8xY?&autoplay=1&loop=1&playlist=JRnPIARW8xY&mute=1&start=215&showinfo=0" title="SAM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
              </a>
            </div>
            <div class="grid"></div>
            <!-- <div class="list-item highlight previews" data-category="highlight">
              <h4> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="https://cybathlon.ethz.ch/de/teams/edan">Cybathlon Challenges </a> (assistant robot race)</h4>
              <h4> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Bayesian Active Learning Demo on <a href="https://www.dlr.de/rm/en/desktopdefault.aspx/tabid-11670/20388_read-47709/">EDAN</h4></a>
              <h4> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Bayesian Active Learning Demo on <a href="https://www.youtube.com/watch?v=ZoNNjQfUdJw&ab_channel=DLRRM">SAM</a></h4>
            </div> -->
          <!-- </p> -->
          </tbody>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <!-- </tbody> -->
        <!-- </table> -->
        </td>
      </tr>

	<!--         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
	          <tbody>
	            <tr>
	            <td style="padding:20px;width:100%;vertical-align:middle">
	              <heading>Pre-prints</heading>
	              <br>(*: equal contribution.)
	            </td>
	          </tr>
	        </tbody>
	      </table>
	        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> -->

          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Publications</heading>
            <br>(*: equal contribution.)
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	<! ------------------------------------------------------- NF4OOD ----------------------------------------------------------------- -->
        <tr >
           <td style="padding:20px;width:25%;vertical-align:middle">
             <div class="one">
               <a href="https://openreview.net/forum?id=BzjLaVvr955"><img src='images/corl23.png' width="250"></a>
               <!-- <embed src="images/GRACE_teaser.pdf" width="190" /> -->
             </div>
           </td>
           <td style="padding:20px;width:75%;vertical-align:middle">
             <a href="https://openreview.net/forum?id=BzjLaVvr955">
               <papertitle>Topology-Matching Normalizing Flows for Out-of-Distribution Detection in Robot Learning</papertitle>
             </a>
             <br>
                  <strong>Jianxiang Feng</strong>,
        	  <a href="https://rmc.dlr.de/rm/en/staff/jongseok.lee/">Jongseok Lee</a>,
                  <a href="https://www.cs.cit.tum.de/en/daml/team/simon-geisler/">Simon Geisler</a>,
	          <a href="https://www.cs.cit.tum.de/en/daml/team/damlguennemann/">Stephan Günnemann</a>,
                  <a href="https://rmc.dlr.de/rm/de/staff/rudolph.triebel/">Rudolph Triebel</a>,
             <br>
             7th Conference on Robot Learning (CoRL) 2023.
             <br>
             <a href="https://github.com/DLR-RM">code</a>
             /   
              <a href="data/corl23.pdf">poster</a>
              /
              <a href="data/corl23_NF_spotlight_1min_v2.mp4">spotlight presentation</a>
              /
              <!-- <a href="images/bal_edan.mp4">video</a>
              / -->
                <a href="data/corl23.bib">bibtex</a>
                <p></p>
                <p>
                We propose to equip NFs with efficient but flexible base distributions to overcome the topological constraint for OOD detection in robot learning.
                </p>
        </tr>
     <! ----------------------------------------------------------------------------------------------------------------------------- -->	

        <! ------------------------------------------------------- RASP ----------------------------------------------------------------- -->
        <tr >
           <td style="padding:20px;width:25%;vertical-align:middle">
             <div class="one">
               <a href="https://arxiv.org/pdf/2303.10135.pdf"><img src='images/GRACE_teaser1.png' width="190"></a>
               <!-- <embed src="images/GRACE_teaser.pdf" width="190" /> -->
             </div>
           </td>
           <td style="padding:20px;width:75%;vertical-align:middle">
             <a href="https://arxiv.org/pdf/2303.10135.pdf">
               <papertitle>Efficient and Feasible Robotic Assembly Sequence Planning via
                 Graph Representation Learning</papertitle>
             </a>
             <br>
             Matan Atad*,
             <strong>Jianxiang Feng*</strong>,
             <a href="https://scholar.google.com.uy/citations?user=NeZae4kAAAAJ&hl=es">Ismael Rodríguez</a>,
             <a href="https://rmc.dlr.de/rm/de/staff/maximilian.durner/">Maximilian Durner</a>,
             <a href="https://rmc.dlr.de/rm/de/staff/rudolph.triebel/">Rudolph Triebel</a>,
             <br>
             IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2023.
             <br>
             <a href="https://github.com/DLR-RM/GRACE">code</a>
             /
             <!-- <a href="https://www.youtube.com/watch?v=EoER_5vYZsU&t=3s&ab_channel=DLRRM">video</a> / -->
             <a href="https://arxiv.org/pdf/2303.10135">arXiv</a>
             <!-- / -->
             <!-- <a href="data/bal2022.bib">bibtex</a> -->
             <p></p>
             <p>
             A holistic graphical approach including a graph representation for product assemblies and a policy architecture, Graph Assembly Processing Network, dubbed GRACE to predict assembly sequences in a step-by-step manner.
             </p>
         </tr>
     <! ----------------------------------------------------------------------------------------------------------------------------- -->	
         
     <! ------------------------------------------------------- NF RASP ----------------------------------------------------------------- -->
             <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <!-- <img src='GRACE_teaser.pdf' width="160" height="2100px" > -->
                    <a href="https://openreview.net/forum?id=Zw9rQMDl8m&referrer=[the%20profile%20of%20Jianxiang%20Feng]"><img src='images/NFs_ASP_teaser.png' width="190" height="120"></a>
                    <!-- <embed src="images/NFs_ASP_teaser.pdf" width="190" /> -->
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://openreview.net/forum?id=Zw9rQMDl8m&referrer=[the%20profile%20of%20Jianxiang%20Feng]">
                    <papertitle>Density-based Feasibility Learning with
                      Normalizing Flows for Introspective Robotic
                      Assembly</papertitle>
                  </a>
                  <br>
                  <strong>Jianxiang Feng*</strong>,
                  Matan Atad*,
                  <a href="https://scholar.google.com.uy/citations?user=NeZae4kAAAAJ&hl=es">Ismael Rodríguez</a>,
                  <a href="https://rmc.dlr.de/rm/de/staff/maximilian.durner/">Maximilian Durner</a>,
	          <a href="https://www.cs.cit.tum.de/en/daml/team/damlguennemann/">Stephan Günnemann</a>,
                  <a href="https://rmc.dlr.de/rm/de/staff/rudolph.triebel/">Rudolph Triebel</a>,
                  <br>
                  Workshop on Robotics and AI: The Future of Industrial Assembly Tasks,
                  Robotics: Science and Systems (RSS) 2023.
                  <br>
                  <a href="https://github.com/DLR-RM/GRACE">code</a>
                  /
                  <!-- <a href="https://www.youtube.com/watch?v=EoER_5vYZsU&t=3s&ab_channel=DLRRM">video</a> / -->
                  <a href="https://arxiv.org/abs/2307.01317">arxiv</a>
                  /
                  <a href="data/rss_ws_nf4rasp_v2.mp4">presentation video</a>
                  /
                  <a href="data/rss_ws_nf4rasp_v2.pdf">slides</a>
                  /
                  <a href="data/rss_ws_2023.bib">bibtex</a>
                  <p></p>
                  <p>
                    A density-based method with normalizing flows for feasibility learning in Robotic Assembly based on only feasible examples.
                  </p>
              </tr>
      <! ----------------------------------------------------------------------------------------------------------------------------- -->	

<! ------------------------------------------------------- BAL EDAN ----------------------------------------------------------------- -->
<!--     <tr onmouseout="bal_edan_stop()" onmouseover="bal_edan_start()"  bgcolor="#ffffd0"> -->
   <tr  >
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <!-- <div class="two" id='bal_edan_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/bal_edan.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div> -->
          <a href="https://arxiv.org/pdf/2109.11547"><img src='images/iros22_bal.png' width="190"></a>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/9982175">
          <papertitle>Bayesian Active Learning for Sim-to-Real Robotic Perception</papertitle>
        </a>
        <br>
        <strong>Jianxiang Feng</strong>,
        <a href="https://rmc.dlr.de/rm/en/staff/jongseok.lee/">Jongseok Lee</a>,
        <a href="https://rmc.dlr.de/rm/de/staff/maximilian.durner/">Maximilian Durner</a>,
        <a href="https://rmc.dlr.de/rm/de/staff/rudolph.triebel/">Rudolph Triebel</a>,
        <br>
        IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2022.
        <br>
        <a href="https://github.com/DLR-RM/BayesSim2Real">code</a>
        /
        <a href="https://arxiv.org/pdf/2109.11547">arXiv</a>
        /
        <a href="data/iros2022_bayesSim2real_v2.pdf">slides</a>
        /
        <a href="data/IROS22_1546.mp4">presentation video</a>
        /
        <a href="images/bal_edan.mp4">demo video</a>
        <!-- <video width="560" height="315" playsinline="" muted="" autoplay="" loop=""><source src="images/bal_edan.mp4" type="video/mp4"></video></a> -->
        /
        <a href="data/bal2022.bib">bibtex</a>
        <p></p>
        <p>
          An active learning pipeline to reduce annotation efforts of real data within a Sim-to-Real scenario via deep Bayesian active learning.
        </p>
    </tr>
<! ----------------------------------------------------------------------------------------------------------------------------- -->	

<! ------------------------------------------------------- ISRR ----------------------------------------------------------------- -->
<!--     <tr onmouseout="bal_edan_stop()" onmouseover="bal_edan_start()"  bgcolor="#ffffd0"> -->
   <tr >
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <!-- <embed src="images/isrr_teaser.pdf" width="190" /> -->
          <a href="https://arxiv.org/pdf/2109.12869"><img src='images/isrr_teaser.png' width="190" height="95"></a>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://link.springer.com/chapter/10.1007/978-3-030-95459-8_40">
          <papertitle>Introspective Robot Perception using Smoothed Predictions from Bayesian Neural Networks</papertitle>
        </a>
        <br>
        <strong>Jianxiang Feng*</strong>,
        <a href="https://rmc.dlr.de/rm/de/staff/maximilian.durner/">Maximilian Durner*</a>,
        <a href="https://scholar.google.com/citations?user=VGBlCk4AAAAJ&hl=en">Zoltán-Csaba Márton</a>,
        <a href="https://ai.uni-bremen.de/team/ferenc_balint-benczedi">Ferenc Bálint-Benczédi</a>,
        <a href="https://rmc.dlr.de/rm/de/staff/rudolph.triebel/">Rudolph Triebel</a>,
        <br>
        The International Symposium of Robotics Research (ISRR) 2019.
        <br>
        <!-- <a href="https://github.com/DLR-RM/BayesSim2Real">code</a>
        /
        <a href="https://www.youtube.com/watch?v=EoER_5vYZsU&t=3s&ab_channel=DLRRM">video</a>
        / -->
        <a href="https://arxiv.org/pdf/2109.12869">arXiv</a>
        /
        <a href="data/isrr19.bib">bibtex</a>
        <p></p>
        <p>
        A method for adaptive image classification based on fusing uncertainty estimates from Bayesian Neural Networks as unary potentials within a Conditional Random Field (CRF).
        </p>
    </tr>
<! ----------------------------------------------------------------------------------------------------------------------------- -->	

  <! ------------------------------------------------------- Survey ----------------------------------------------------------------- -->
  <!--     <tr onmouseout="bal_edan_stop()" onmouseover="bal_edan_start()"  bgcolor="#ffffd0"> -->
     <tr >
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <a href="https://arxiv.org/abs/2107.03342"><img src='images/survey.png' width="190"></a>
            <!-- <embed src="images/survey1.pdf" width="190" /> -->
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://link.springer.com/article/10.1007/s10462-023-10562-9">
            <papertitle>A Survey of Uncertainty in Deep Neural Networks </papertitle>
          </a>
          <br>
          <a href="https://scholar.google.com/citations?user=JSDN9rsAAAAJ&hl=en">Jakob Gawlikowski</a>,
          Cedrique Rovile Njieutcheu Tassi, 
          Mohsin Ali, 
          <a href="https://rmc.dlr.de/rm/en/staff/jongseok.lee/">Jongseok Lee</a>,
          <a href="https://www.hummat.com/">Matthias Humt</a>,
          <strong>Jianxiang Feng</strong>,
          Anna Kruspe, 
          <a href="https://rmc.dlr.de/rm/de/staff/rudolph.triebel/">Rudolph Triebel</a>,
          Peter Jung, 
          Ribana Roscher, 
          Muhammad Shahzad, 
          Wen Yang, 
          Richard Bamler, 
          <a href="https://www.professoren.tum.de/en/zhu-xiaoxiang"> Xiaoxiang Zhu</a>
          <br> 
		Artificial Intelligence Review (2023): 1-77
          <br>
          	<a href="https://arxiv.org/abs/2107.03342">arXiv</a>
	        /
          <a href="data/survey.bib">bibtex</a> 
          <p></p>
          <p>
           A comprehensive overview of uncertainty estimation in neural networks, reviewing recent advances in the field, highlighting current challenges, and identifying potential research opportunities. 
          </p>
      </tr>
  <! ----------------------------------------------------------------------------------------------------------------------------- -->	
	      
	      
<! ------------------------------------------------------- CoRL21 ----------------------------------------------------------------- -->
<!--     <tr onmouseout="bal_edan_stop()" onmouseover="bal_edan_start()"  bgcolor="#ffffd0"> -->
   <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <!-- <embed src="images/corl21.pdf" width="190" /> -->
          <a href="https://proceedings.mlr.press/v164/lee22c.html"><img src='images/corl21.png' width="190" height="130"></a>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://proceedings.mlr.press/v164/lee22c.html">
          <papertitle>Trust Your Robots! Predictive Uncertainty Estimation of Neural Networks with Sparse Gaussian Processes</papertitle>
        </a>
        <br>
        <a href="https://rmc.dlr.de/rm/en/staff/jongseok.lee/">Jongseok Lee</a>,
        <strong>Jianxiang Feng</strong>,
        <a href="https://www.hummat.com/">Matthias Humt</a>,
        <a href="https://rmc.dlr.de/rm/de/staff/marcus.mueller/">Marcus G. Muller</a>,
        <a href="https://rmc.dlr.de/rm/de/staff/rudolph.triebel/">Rudolph Triebel</a>,
        <br>
        5th Conference on Robot Learning (CoRL) 2021.
        <br>
        <a href="https://github.com/DLR-RM/moegplib">code</a>
        /
        <a href="https://www.youtube.com/watch?v=vu2TnDEqDRk&t=37s&ab_channel=DLRRM">video</a>
        /
        <!-- <a href="https://arxiv.org/pdf/2109.12869">arXiv</a>
        / -->
        <a href="data/pmlr-v164-lee22c.bib">bibtex</a>
        <p></p>
        <p>
        A probabilistic framework to obtain both reliable and fast uncertainty estimates for predictions with Deep Neural Networks (DNNs) based on Sparse Gaussian Processes. 
        </p>
    </tr>
<! ----------------------------------------------------------------------------------------------------------------------------- -->	

<! ------------------------------------------------------- ICML2020 ----------------------------------------------------------------- -->
<!--     <tr onmouseout="bal_edan_stop()" onmouseover="bal_edan_start()"  bgcolor="#ffffd0"> -->
   <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
            <!-- <embed src="images/icml20.pdf" width="190" /> -->
            <a href="http://proceedings.mlr.press/v119/lee20b.html"><img src='images/icml20.png' width="190" height="180"></a>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="http://proceedings.mlr.press/v119/lee20b.html">
          <papertitle>Estimating Model Uncertainty of Neural Networks in Sparse Information Form  </papertitle>
        </a>
        <br>
        <a href="https://rmc.dlr.de/rm/en/staff/jongseok.lee/">Jongseok Lee</a>,
        <a href="https://www.hummat.com/">Matthias Humt</a>,
        <strong>Jianxiang Feng</strong>,
        <a href="https://rmc.dlr.de/rm/de/staff/rudolph.triebel/">Rudolph Triebel</a>,
        <br>
        International Conference on Machine Learning (ICML) 2020.
        <br>
        <a href="https://github.com/DLR-RM/curvature">code</a>
        /
        <!-- <a href="https://www.youtube.com/watch?v=vu2TnDEqDRk&t=37s&ab_channel=DLRRM">video</a>
        / -->
        <!-- <a href="https://arxiv.org/pdf/2109.12869">arXiv</a>
        / -->
        <a href="data/pmlr-v119-lee20b.bib">bibtex</a>
        <p></p>
        <p>
         A sparse representation of model uncertainty for Deep Neural Networks (DNNs) where the parameter posterior is approximated with an inverse formulation of the Multivariate Normal Distribution (MND), also known as the information form. 
        </p>
    </tr>
<! ----------------------------------------------------------------------------------------------------------------------------- -->	

<! ------------------------------------------------------- SAM ----------------------------------------------------------------- -->
<!--    <tr onmouseout="FR_SAM_stop()" onmouseover="FR_SAM_start()"  bgcolor="#ffffd0"> -->
  <tr >
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <!-- <div class="two" id='FR_SAM_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/bal_edan.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div> -->
          <a href="https://elib.dlr.de/194517/1/Vol3_10.pdf"><img src='images/FR2023.png' width="190"></a>
        </div>
      </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://elib.dlr.de/194517/1/Vol3_10.pdf">
        <papertitle>Virtual Reality via Object Pose Estimation and Active Learning: Realizing Telepresence Robots with Aerial Manipulation Capabilities</papertitle>
      </a>
      <br>
      <a href="https://rmc.dlr.de/rm/en/staff/jongseok.lee/">Jongseok Lee</a>, <a href="https://rmc.dlr.de/rm/en/staff/ribin.balachandran/">Ribin Radhakrishna Balachandran</a> , <a href="https://de.linkedin.com/in/konstantin-kondak-02265526">Konstantin Kondak</a>, <a href="https://scholar.google.com.br/citations?user=HZlxXRYAAAAJ&hl=en">Andre Coelho</a>, <a href="https://rmc.dlr.de/rm/de/staff/marco.destefano/">Marco De Stefano</a>, <a href="https://www.hummat.com/">Matthias Humt</a>, <strong>Jianxiang Feng</strong>, <a href="https://h2t.anthropomatik.kit.edu/21_2372.php">Tamim Asfour</a>,   <a href="https://rmc.dlr.de/rm/de/staff/rudolph.triebel/">Rudolph Triebel</a>,
    
      <br>
      Field Robotics
      <br>
      <a href="https://www.youtube.com/watch?v=JRnPIARW8xY&ab_channel=DLRRM">video</a>
      /
      <a href="data/fr2023.bib">bibtex</a>
      <!-- /
      <a href="data/bal2022.bib">bibtex</a> -->
      <p></p>
      <p>
        A novel teleoperation system for advancing aerial manipulation in dynamic and unstructured environments based on pose estimation pipelines for the industrial objects of
        both known and unknown geometries and an active learning pipeline.
      </p>
    </td>
  </tr>
  <! ------------------------------------------------------------------------------------------------------------------------ -->		
  </tbody></table>

				
	<! ------------------------------------------------------- Academic Services ---------------------------------------------------------- -->	
  
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Academic Services</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
		
	    <div class="list-item highlight" data-category="highlight">
              <p class="date">2023</p> &nbsp;&nbsp;&nbsp; Reviewer, CoRL.
            </div>
		    
            <div class="list-item highlight" data-category="highlight">
              <p class="date">2022</p> &nbsp;&nbsp;&nbsp; Reviewer, ICRA, CoRL, IROS.
            </div>
          
            <div class="list-item highlight" data-category="highlight">
              <p class="date">2020</p> &nbsp;&nbsp;&nbsp; Reviewer, ICRA, IROS, ECAI.
            </div>
              
            
            </td>
          <!-- </tr> -->
      
	<! ------------------------------------------------------------------------------------------------------------------------ -->		
				
	<! ------------------------------------------------------- Events ---------------------------------------------------------- -->	
  
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Events </heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
		
		<div class="list-item highlight" data-category="highlight">
                <p class="date">06.2023</p> &nbsp;&nbsp;&nbsp; Team player, the <a href=https://www.linkedin.com/feed/update/urn:li:activity:7091522148384550913/?origin=SHARED_BY_YOUR_NETWORK>aerial manipulation demo</a> for <a href="https://www.kuka.com/en-us/future-production/research-and-development/kuka-innovation-award/kuka-innovation-award-2023">kuka inovation award 2023</a> in <a href="https://automatica-munich.com/en/">Automatica</a> 2023 (<a href="https://www.linkedin.com/posts/kukaglobal_team-jarvis-from-the-merlin-labodratory-of-activity-7080170709481574400-7guO?utm_source=share&utm_medium=member_desktop">press</a>).
                </div><div class="list-item highlight" data-category="highlight">
			
	    <div class="list-item highlight" data-category="highlight">
		 <p class="date">03.2023</p> &nbsp;&nbsp;&nbsp; Team player, the 1st place in the discipline assist robot race of <a href="https://cybathlon.ethz.ch/de/teams/edan">Cybathlon Challenge</a> as part of EDAN team (<a href="https://www.dlr.de/rm/en/desktopdefault.aspx/tabid-3755/17612_read-81519/year-2023/">press</a>, <a href="https://www.youtube.com/watch?v=EoER_5vYZsU&t=3s&ab_channel=DLRRM">video</a>)
            </div>
			
          <div class="list-item highlight" data-category="highlight">
              <p class="date">06.2022</p> &nbsp;&nbsp;&nbsp; Team player, <a href="https://www.dlr.de/rm/desktopdefault.aspx/tabid-11670/#gallery/28208">EDAN</a> demo at Automatica Exhibition 2022 ( <a href="https://www.dlr.de/rm/en/desktopdefault.aspx/tabid-3755/17612_read-77446/year-all/17612_page-3/#/gallery/37616">press</a>).
            </div>

            <div class="list-item highlight" data-category="highlight">
            <p class="date">10.2022</p> &nbsp;&nbsp;&nbsp; Co-Organizer, IROS Workshop on <a href="https://probabilisticrobotics.github.io/2022/">Probabilistic Robotics in the age of Deep Learning</a>.
            </div>
              
            
            </td>
          <!-- </tr> -->
      
	<! ------------------------------------------------------------------------------------------------------------------------ -->		
				
	<! ------------------------------------------------------- Misc ---------------------------------------------------------- -->	
  
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Misc</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
		
            <div class="list-item highlight" data-category="highlight">
            <p class="date">03.2023</p> &nbsp;&nbsp;&nbsp; Master Thesis Supervision: <a href="https://elib.dlr.de/194212/">"Graph Neural Networks for Knowledge Transfer in Robotic Assembly Sequence Planning"</a> by Matan Atad, co-supervision with <a href="https://rmc.dlr.de/rm/de/staff/maximilian.durner/">Maximilian Durner</a>, <a href="https://rmc.dlr.de/rm/en/staff/ismael.rodriguezbrena/index.html">Ismael Rodriguezbrena</a>.
           </div>  
           <br>
          
            <div class="list-item highlight" data-category="highlight">
              <p class="date">11.2022</p> &nbsp;&nbsp;&nbsp; Master Thesis Supervision: <a href="https://elib.dlr.de/194212/">"Scene Graph Generation from Visual perception for Task and Motion Planning"</a> by Mohit Kumar, co-supervision with <a href="https://rmc.dlr.de/rm/en/staff/samuel.bustamante/">Samuel Bustamante</a>.
            </div>  
            <br>
            
            </td>
          <!-- </tr> -->
      
	<! ------------------------------------------------------------------------------------------------------------------------ -->				
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://github.com/jonbarron/jonbarron_website">Link to the template</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

  <script>
        function toggle_bio() {
        var x = document.getElementById("more-bio");
        if (x.style.display === "none") {
          x.style.display = "block";
        } else {
          x.style.display = "none";
        }
      }
  </script>
</body>

</html>
