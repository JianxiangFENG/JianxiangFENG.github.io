<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jianxiang Feng</title>
  
  <meta name="author" content="Jianxiang Feng">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="images/favicon.ico">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jianxiang Feng | 冯健祥</name>
              </p>
              <p>Hi there! I received my Ph.D. from <a href="https://www.tum.de/en">Technical University of Munich (TUM)</a> and the <a href="https://www.dlr.de/rm/en/desktopdefault.aspx/tabid-8017/">Institute of Robotic and Mechtronic</a> (RM), <a href="https://www.dlr.de/en/">German Aerospace Center</a> (DLR). 
		      I was advised by Prof. <a href="https://scholar.google.com/citations?user=SuOUxjUAAAAJ&hl=en"> Rudolph Triebel</a> (DLR&KIT) and affiliated with <a href="https://www.mu-ds.de/"> Munich School of Data Science</a>, where I worked with Prof. <a href="https://www.cs.cit.tum.de/en/daml/team/damlguennemann/">Stephan Günnemann</a>. 
		      Recently, I am a senior research scientist at <a href="https://www.agile-robots.com/en/"> Agile Robots SE</a>, investigating learning-based manipulation, e.g., vision-language-action models (VLAs), dexterous grapsing, etc..
              </p>
              <p>
                In general, my research interests reside in the intersection of robotics and machine learning. A primary focus is on the <strong>trustworthy</strong> and <strong>adaptable</strong> learning ability of a robot in an <strong>open-world</strong> environment. Aiming to equip an autonomous agent with <strong>introspective</strong> capabilities i.e., uncertainty quantification/awareness of system internal states, I am passionate about investigating <strong>probabilistic</strong> machine learning on how to properly model/quantify it and leverage such information for <strong>learning-enabled robotics</strong>. 
                <!-- I believe that this can lead to more explainable and ultimately safer autonomous robot systems. -->
              </p>
	      <p> P.S. The pronunciation of my first name "Jianxiang" is quite close to "Jensen" (yep, the well-known "Jensen inequality" often used in the Evidence Lower Bound (ELBO) derivation) but with a different ending :P. 
	      </p>
		    
              <div id="more-bio" style="display: None">
                <p>Jianxiang Feng received his PhD in robotics and machine learning from Technical University of Munich (TUM) and the Institute of Robotics and Mechatronics (RM), the German Aerospace Center (DLR) in 2024. Before he obtained his M.Sc in Electrical Engineering and Information Technology from TUM, 2019 and B.Sc in Electronic Engineering from Beijing University of Posts and Telecommunication (BUPT), 2015. His research interests reside in the intersection of robotics and machine learning.</p>
              </div>
              <p style="text-align:center">
                <a href="javascript:toggle_bio()">Short Bio</a>&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://drive.google.com/file/d/1GuVC6gXGBkzs1LMAGm0i1AsWvT9m4FvT/view?usp=sharing">CV</a> &nbsp;&nbsp;&nbsp;&nbsp;
<!-- 			/<a href="https://drive.google.com/file/d/178MR_pM2WFGkKol1vmE6DO13ew30V__z/view?usp=sharing">Research</a></a>&nbsp;&nbsp;&nbsp;&nbsp; -->
                <a href="https://github.com/JianxiangFENG">Github</a>&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://scholar.google.com/citations?user=b-5CscIAAAAJ&hl=en">G. Scholar</a> &nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://www.linkedin.com/in/jianxiangfeng/">LinkedIn</a>&nbsp;&nbsp;&nbsp;&nbsp;
<!--                 <a href="https://twitter.com/fengjianxiang?lang=en">Twitter</a>&nbsp;&nbsp;&nbsp;&nbsp; -->
                <br><br>
<!--                 jianxiang.feng at tum dot de -->
                <br><br>
              </p>
		
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/profile-half.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
  <!-- <td style="padding:20px;width:100%;vertical-align:middle"> -->
    <h1 style="font-size: 30px">News</h1>
    <p>
      <div class="list-item highlight" data-category="highlight">
    <p class="date">10.2025</p> &nbsp;&nbsp;&nbsp; Invited Talk at IROS 2025 Workshop on <a href="https://activep-ws.github.io/">Advancing Active Perception</a>. </div><div class="list-item highlight" data-category="highlight">
      <p></p>    
    <p class="date">08.2025</p> &nbsp;&nbsp;&nbsp; One paper (<a href="https://arxiv.org/abs/2503.03890">LensDFF</a>) accepted for IROS 2025 and another one (<a href="https://sites.google.com/view/ffhflow/home/"">FFHFlow</a>) for CoRL 2025. </div><div class="list-item highlight" data-category="highlight">
      <p></p>    
    <p class="date">07.2025</p> &nbsp;&nbsp;&nbsp; Invited Talk by the institute of Artificial Intelligence, China Telecom (<a href="https://www.linkedin.com/company/the-institute-of-artificial-intelligence-china-telecom-teleai/about/">TeleAI</a>) at <a href="https://www.worldaic.com.cn/">World AI Conference</a>, Shanghai.</div><div class="list-item highlight" data-category="highlight">
      <p></p>    
    <p class="date">03.2025</p> &nbsp;&nbsp;&nbsp; One <a href="https://www.nature.com/articles/s41598-025-89405-2">paper</a> on the <a href="https://www.dlr.de/en/rm/research/robotic-systems/mobile-platforms/edan">DLR EDAN system</a> is available on <a href="https://www.nature.com/srep/">Nature Scientfic Reports</a>. </div><div class="list-item highlight" data-category="highlight">
      <p></p>    
    <p class="date">01.2025</p> &nbsp;&nbsp;&nbsp; One <a href="https://arxiv.org/pdf/2407.00451">paper</a> on 3D object-centric imitation learning accepted for ICRA 2025. </div><div class="list-item highlight" data-category="highlight">
      <p></p>              
  <p class="date">12.2024</p> &nbsp;&nbsp;&nbsp; Thanks to the support from my thesis committee, I finally defended my <a href="https://mediatum.ub.tum.de/1729794">doctoral thesis</a> and earned my Ph.D.! </div><div class="list-item highlight" data-category="highlight">
    <p></p>
	      <p class="date">10.2024</p> &nbsp;&nbsp;&nbsp; Congratulations to my former colleagues in <a href="https://cybathlon.ethz.ch/de/teams/edan">DLR EDAN team</a> for winning in the discipline of Assistance Robot Race at <a href="https://cybathlon.ethz.ch/en/event/disciplines/rob">Cybathlon 2024</a>! </div><div class="list-item highlight" data-category="highlight">
		<p></p>
	      <p class="date">09.2024</p> &nbsp;&nbsp;&nbsp; Two papers on <a href="https://arxiv.org/abs/2407.17348">learning-based grasping</a> and <a href="https://arxiv.org/abs/2310.17923">its application in dynamic scenarios</a> accepted for <a href="https://2024.ieee-humanoids.org//">IEEE-RAS Humanoids 2024</a>. </div><div class="list-item highlight" data-category="highlight">
		<p></p>
		      
	      <p class="date">04.2024</p> &nbsp;&nbsp;&nbsp; Our recent <a href="https://sites.google.com/view/konwloop/home">investigation</a> on trustworthy closed-loop LLM-based planners accepted for the <a href="https://probabilisticrobotics.github.io/">Workshop on Probabilistic Robot Learning</a> at ICRA 2024. </div><div class="list-item highlight" data-category="highlight">
		<p></p>    
              <div class="list-item highlight" data-category="highlight">
	      <p class="date">01.2024</p> &nbsp;&nbsp;&nbsp; Our ICRA 2024 <a href="https://probabilisticrobotics.github.io/">Workshop</a> titled "Back to the Future: Robot Learning Going Probabilistic" got accepted, thrilled to co-organize it! Stay tuned!</div><div class="list-item highlight" data-category="highlight">
		<p></p>    
              <div class="list-item highlight" data-category="highlight">
	      <p class="date">08.2023</p> &nbsp;&nbsp;&nbsp; A <a href="https://openreview.net/forum?id=BzjLaVvr955&referrer=%5Bthe%20profile%20of%20Jianxiang%20Feng%5D(%2Fprofile%3Fid%3D~Jianxiang_Feng1">paper</a> on Normalizing Flows for Out-of-Distribution Detection accepted for CoRL 2023!
              </div><div class="list-item highlight" data-category="highlight">
		<p></p>      
              <p class="date">07.2023</p> &nbsp;&nbsp;&nbsp; A <a href="https://arxiv.org/pdf/2303.10135.pdf">paper</a> on Graph Neural Networks and Assembly Sequence Planning accepted for IROS 2023.
              </div><div class="list-item highlight" data-category="highlight">
		<p></p>
                <div class="list-item highlight" data-category="highlight">
                <p class="date">06.2023</p> &nbsp;&nbsp;&nbsp; Our <a href=https://www.youtube.com/watch?v=FS8Hcwu6N7I&ab_channel=KUKA-Robots%26Automation>aerial manipulation demo</a> for <a href="https://www.kuka.com/en-us/future-production/research-and-development/kuka-innovation-award/kuka-innovation-award-2023">kuka inovation award 2023</a> gained immense popularity at <a href="https://automatica-munich.com/en/">Automatica</a> 2023 (<a href="https://www.linkedin.com/posts/kukaglobal_team-jarvis-from-the-merlin-labodratory-of-activity-7080170709481574400-7guO?utm_source=share&utm_medium=member_desktop">press</a>).
                </div><div class="list-item highlight" data-category="highlight">
		<p></p>
              <div class="list-item highlight" data-category="highlight">
              <p class="date">06.2023</p> &nbsp;&nbsp;&nbsp; A <a href="https://arxiv.org/abs/2307.01317">paper</a> on assembly feasibility learning accepted for the <a href="https://sites.google.com/nvidia.com/industrial-assembly">workshop on Robotics and AI: The Future of Industrial Assembly Tasks</a> at RSS 2023.
              </div><div class="list-item highlight" data-category="highlight">
		<p></p>
	    <div class="list-item highlight" data-category="highlight">
		 <p class="date">03.2023</p> &nbsp;&nbsp;&nbsp; Won the 1st place in the discipline of Assistance Robot Race of <a href="https://www.dlr.de/en/rm/latest/news/2023/participation-in-cybathlon-challenges-2023">Cybathlon Challenge</a> as part of <a href="https://www.youtube.com/watch?v=EoER_5vYZsU&t=3s&ab_channel=DLRRM">DLR EDAN team</a>. 
            </div>
            </p>
          <!-- </td> -->
        </tr>
      </tbody></table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
        <tr>
        <!-- <td style="padding:20px;width:100%;vertical-align:middle"> -->
          <h1 style="font-size: 30px">Videos</h1>
        <!-- <p>
          <h4> &emsp;&emsp;&emsp;&emsp; <a href="https://cybathlon.ethz.ch/de/teams/edan">Cybathlon Challenges </a> (assistant robot race) 
            &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;  Bayesian Active Learning Demo on <a href="https://www.dlr.de/rm/en/desktopdefault.aspx/tabid-11670/20388_read-47709/">EDAN</a>
            &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Bayesian Active Learning Demo on <a href="https://www.youtube.com/watch?v=ZoNNjQfUdJw&ab_channel=DLRRM">SAM</a></h4>
          </p> -->
        <p></p>
        <p></p>
        <div class="list-item highlight previews" data-category="highlight">

          <a href="https://sites.google.com/view/ffhflow/home/"><iframe width="340" height="210" src="https://www.youtube.com/embed/QYa1qN_Sl9U?si=7Z3d4X_o8DGW8qh3&autoplay=1&loop=1&playlist=QYa1qN_Sl9U&mute=1&start=31&showinfo=0" title="FFHFlow" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
          </a> 
          <!-- <h4> CYBATHLON Challenges (assistant robot race)</h4> -->
          <!-- <h4> Bayesian Active Learning Demo on an assitive robot EDAN</h4> -->
          <a href="https://arxiv.org/pdf/2109.11547"><iframe width="340" height="210" src="https://www.youtube.com/embed/kfWNEBbvuSE?&autoplay=1&loop=1&playlist=kfWNEBbvuSE&mute=1&start=31&showinfo=0" title="Edan" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
          </a> 
          <a href="https://www.youtube.com/watch?v=EoER_5vYZsU&t=3s&ab_channel=DLRRM"><iframe width="340" height="210" src="https://www.youtube.com/embed/EoER_5vYZsU?&autoplay=1&loop=1&playlist=EoER_5vYZsU&mute=1&start=100&showinfo=0" title="Cybathlon" frameborder="0" allow="accelerometer; allow='autoplay'; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
          </a>  
          <!--  <h4> Bayesian Active Learning Demo on an assitive robot EDAN</h4>
          <a href="https://arxiv.org/pdf/2109.11547"><video width="560" height="315" playsinline="" muted="" autoplay="" loop=""><source src="images/bal_edan.mp4" type="video/mp4"></video></a> -->
    
          </div>
          <p></p>
          <br>
          <br>
          
          <!-- <div class="grid"></div> -->
          <!-- <div class="grid"></div> -->
          
          <div class="list-item highlight previews" data-category="highlight">
          
          <a href="https://youtu.be/MTxhxTC660k"><iframe width="340" height="210" src="https://www.youtube.com/embed/MTxhxTC660k?si=qoHbAwDsPZmsP6M2&autoplay=1&loop=1&playlist=MTxhxTC660k&mute=1&start=13&showinfo=0" title="LensDFF" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
          </a> 
          
          <!-- <h4> OOD detection Demo on aerial manipulation robnot SAM</h4> -->
          <a href="https://www.youtube.com/watch?v=L09z-AuRI0A"><iframe width="340" height="210" src="https://www.youtube.com/embed/L09z-AuRI0A?si=qcZgZu5TWW5P7FGT?&autoplay=1&loop=1&playlist=L09z-AuRI0A&mute=1&start=10&showinfo=0" title="ood_sam" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
          </a>

          <!-- <h4> Bayesian Active Learning Demo on aerial manipulation robot SAM</h4> -->
          <a href="https://www.youtube.com/watch?v=JRnPIARW8xY&ab_channel=DLRRM"><iframe width="340" height="210" src="https://www.youtube.com/embed/JRnPIARW8xY?&autoplay=1&loop=1&playlist=JRnPIARW8xY&mute=1&start=215&showinfo=0" title="SAM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
          </a>
          </div>

          <div class="grid"></div>

        <!-- <div class="list-item highlight previews" data-category="highlight">
          <h4> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="https://cybathlon.ethz.ch/de/teams/edan">Cybathlon Challenges </a> (assistant robot race)</h4>
          <h4> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Bayesian Active Learning Demo on <a href="https://www.dlr.de/rm/en/desktopdefault.aspx/tabid-11670/20388_read-47709/">EDAN</h4></a>
          <h4> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Bayesian Active Learning Demo on <a href="https://www.youtube.com/watch?v=ZoNNjQfUdJw&ab_channel=DLRRM">SAM</a></h4>
        </div> -->
      <!-- </p> -->
      </tbody>
    </table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <!-- </tbody> -->
    <!-- </table> -->
    <!-- </td> -->
  <!-- </tr> -->

 
		
<! ------------------------------------------------------- Pre-prints ----------------------------------------------------------------- -->
<!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
    <tr>
      <h1 style="font-size: 30px">Preprints</h1>
      (*: equal contribution.)
      <br>
      <br>
      <br>
      <br>
      <br>
  </tr>
</tbody>
</table> -->
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
</tbody></table>
<! ------------------------------------------------------- End of Pre-prints ----------------------------------------------------------------- -->

              
<! ------------------------------------------------------- Publications ----------------------------------------------------------------- -->
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
    <!-- <td style="padding:20px;width:100%;vertical-align:middle"> -->
      <br>
      <br>
      <br>
      <h1 style="font-size: 30px">Publications</h1>
      (*: equal contribution. Some papers are <span class="highlight">highlighted</span>.)
      <br>
      <br>
      <br>
  </tr>
</tbody></table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
<! ------------------------------------------------------- FFHFlow ----------------------------------------------------------------- -->
  <tr bgcolor="#ffffd0">
    <td style="padding:0px;width:25%;vertical-align:middle" >
      <div class="one">
        <a href="https://arxiv.org/abs/2407.15161"><img src='images/ffhflow_v2.png' width="300" height="220"  style="vertical-align:middle;margin:-30px 10px"></a>
      </div>
    </td>
    <td style="padding:60px;width:75%;height:100%;vertical-align:middle">
      <a href="https://arxiv.org/abs/2407.15161">
        <papertitle>FFHFlow: Diverse and Uncertainty-Aware Dexterous Grasp Generation via Flow Variational Inference</papertitle>
      </a>
      <br>
            <strong>Jianxiang Feng</strong>*,
            <a href="https://www.ce.cit.tum.de/air/people/qian-feng-msc/">Qian Feng</a>*,
            <a href="https://dblp.org/pid/63/8729.html">Zhaopeng Chen</a>,
            <a href="https://scholar.google.com/citations?user=SuOUxjUAAAAJ&hl=en">Rudolph Triebel</a>,
            <a href="https://www.ce.cit.tum.de/air/people/prof-dr-ing-habil-alois-knoll/"> Alois Knoll</a>,
      <br>
      9th Conference on Robot Learning (CoRL) 2025.
      <br>
      <a href="https://arxiv.org/abs/2407.15161">arxiv</a> /
      <a href="https://sites.google.com/view/ffhflow/home/">website</a>
          <p></p>
          <p>
          A flow-based variational framework that generates diverse, robust multi-finger grasps while explicitly quantifying perceptual uncertainty in partial point clouds and itself.
          </p>
          </p>
  </tr>
<! ------------------------------------------------------- End of FFHFlow ----------------------------------------------------------------- -->

<! ------------------------------------------------------- LensDFF ----------------------------------------------------------------- -->
  <tr >
    <td style="padding:0px;width:25%;vertical-align:middle">
      <div class="one">
        <a href="https://arxiv.org/abs/2503.03890"><img src='images/lensDFF.png' width="310" height="180"  style="vertical-align:middle;margin:-20px 0px"></a>
      </div>
    </td>
    <td style="padding:60px;width:75%;vertical-align:middle">
      <a href="https://arxiv.org/abs/2503.03890">
        <papertitle>LensDFF: Language-enhanced Sparse Feature Distillation for Efficient Few-Shot Dexterous Manipulation</papertitle>
      </a>
      <br>
            <a href="https://www.ce.cit.tum.de/air/people/qian-feng-msc/">Qian Feng</a>*,
            <a href="https://www.ce.cit.tum.de/air/people/prof-dr-ing-habil-alois-knoll/"> Alois Knoll*</a>,
            <a href="https://david-s-martinez.github.io/"> S. Martinez Lema</a>*,
            <a href="https://dblp.org/pid/63/8729.html">Zhaopeng Chen</a>,
            <strong>Jianxiang Feng</strong>,
      <br>
      IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2025.
      <br>
      <a href="https://arxiv.org/abs/2503.03890">arxiv</a> /
      <a href="https://youtu.be/MTxhxTC660k">video</a>
          <p></p>
          <p>
          An efficient way to distill view-consistent 2D features onto 3D points based on the proposed language-enhanced feature fusion strategy, thereby enabling single-view few-shot generalization.
          </p>
  </tr>
<! ------------------------------------------------------- End of LensDFF ----------------------------------------------------------------- -->

<! ------------------------------------------------------- EDAN ----------------------------------------------------------------- -->
  <tr >
    <td style="padding:0px;width:25%;vertical-align:middle">
      <div class="one">
        <a href="https://www.nature.com/articles/s41598-025-89405-2"><img src='images/edan.png' width="320" height="190"  style="vertical-align:middle;margin:-20px -10px"></a>
      </div>
    </td>
    <td style="padding:60px;width:75%;vertical-align:middle">
      <a href="https://www.nature.com/articles/s41598-025-89405-2">
        <papertitle>An assistive robot that enables people with amyotrophia to perform sequences of everyday activities</papertitle>
      </a>
      <br>
            <a href="https://scholar.google.de/citations?user=fLRUwzkAAAAJ&hl=de">Annette Hagengruber</a>,
            <a href="https://scholar.google.de/citations?user=ngO7dNAAAAAJ&hl=de">Gabriel Quere</a>, 
            <a href="https://scholar.google.de/citations?user=Z9Vpu7MAAAAJ&hl=de">Maged Iskandar</a>, 
            <a href="https://scholar.google.de/citations?user=G-N2GGUAAAAJ&hl=de">Samuel Bustamante</a>, 
            <strong>Jianxiang Feng</strong>,
            <a href="https://scholar.google.de/citations?user=KTd8w14AAAAJ&hl=de">Daniel Leidner</a>, 
            <a href="https://scholar.google.de/citations?user=yP_zL3gAAAAJ&hl=de">Alin Albu-Schäffer</a>,
            <a href="https://scholar.google.de/citations?user=aHPX6PsAAAAJ&hl=de">Freek Stulp</a>,
            <a href="https://scholar.google.de/citations?user=sCFZysQAAAAJ&hl=de">Jörn Vogel</a>,
      <br>
      Nature Portfolio Scientific Reports, 2025.
      <br>
      <a href="https://www.nature.com/articles/s41598-025-89405-2">paper</a> 
          <p></p>
          <p>
          A highly integrated assistive robot EDAN, operated by an interface based on bioelectrical signals, combined with shared control and a whole-body coordination of the entire system, through a case study involving people with motor impairments to accomplish real-world activities..
          </p>
  </tr>
<! ------------------------------------------------------- End of EDAN ----------------------------------------------------------------- -->


<! ------------------------------------------------------- Lan-o3dp ----------------------------------------------------------------- -->
  <tr >
    <td style="padding:0px;width:25%;vertical-align:middle">
      <div class="one">
        <a href="https://arxiv.org/abs/2407.00451"><img src='images/lan_o3dp_pipeline.png' width="340" height="170"  style="vertical-align:middle;margin:-10px -20px"></a>
        <!-- <embed src="images/GRACE_teaser.pdf" width="190" /> -->
      </div>
    </td>
    <td style="padding:60px;width:75%;vertical-align:middle">
      <a href="https://arxiv.org/abs/2407.00451">
        <papertitle>Generalizable Robotic Manipulation: Object-Centric Diffusion Policy with Language Guidance</papertitle>
      </a>
      <br>
            Hang Li,
            <a href="https://www.ce.cit.tum.de/air/people/qian-feng-msc/">Qian Feng</a>,
            Zhi Zheng,
            <strong>Jianxiang Feng</strong>,
            <a href="https://www.ce.cit.tum.de/air/people/prof-dr-ing-habil-alois-knoll/"> Alois Knoll</a>,
      <br>
      IEEE International Conference on Robotics and Automation (ICRA) 2025.
      <br>
      Workshop on Semantics for Robotics: From Environment Understanding and Reasoning to Safe Interaction of Robotics,
      Robotics: Science and Systems (RSS-WS) 2024.
      <br>
      <a href="https://arxiv.org/abs/2407.00451">arxiv</a> /
      <a href="data/lan_o3dp.mp4">video</a>
          <p></p>
          <p>
          A language-guided object-centric diffusion policy that takes a 3d representation of task-relevant objects as conditional input and can be guided by cost function for collision avoidance at inference time.
          </p>
  </tr>
<! ------------------------------------------------------- End of Lan-o3dp ----------------------------------------------------------------- -->


 <! ------------------------------------------------------- DexGANGrasp ----------------------------------------------------------------- -->
  <tr >
    <td style="padding:0px;width:25%;vertical-align:middle">
      <div class="one">
        <a href="https://arxiv.org/abs/2407.17348"><img src='images/dexgangrasp.png' width="300" height="200"  style="vertical-align:middle;margin:-30px 0px"></a>
      </div>
    </td>
    <td style="padding:60px;width:75%;vertical-align:middle">
      <a href="https://arxiv.org/abs/2407.17348">
        <papertitle>DexGANGrasp: Dexterous Generative Adversarial Grasping Synthesis for Task-Oriented Manipulation</papertitle>
      </a>
      <br>
            <a href="https://www.ce.cit.tum.de/air/people/qian-feng-msc/">Qian Feng</a>*,
            <a href="https://david-s-martinez.github.io/">S. Martinez Lema</a>*,
            Mohammadhossein Malmir, Hang Li,
            <strong>Jianxiang Feng</strong>,
            <a href="https://dblp.org/pid/63/8729.html">Zhaopeng Chen</a>,
            <a href="https://www.ce.cit.tum.de/air/people/prof-dr-ing-habil-alois-knoll/"> Alois Knoll</a>,
      <br>
  IEEE-RAS International Conference on Humanoid Robots (Humanoids), 2024.
      <br>
      <a href="https://david-s-martinez.github.io/DexGANGrasp.io/">website</a> /
      <a href="https://arxiv.org/abs/2407.17348">arxiv</a> /
      <a href="https://www.youtube.com/watch?v=egQaemeAy5k&ab_channel=DavidMartinez">video</a>
          <p></p>
          <p>
          A Conditional Generative Adversarial Networks (cGANs)-based DexGenerator to generate dexterous grasps and a discriminator-like DexEvalautor to assess the stability of these grasps.
          </p>
  </tr>
<! ------------------------------------------------------- End of DexGANGrasp ----------------------------------------------------------------- -->

 <! ------------------------------------------------------- Dynamic Grasping ----------------------------------------------------------------- -->
  <tr >
    <td style="padding:0px;width:25%;vertical-align:middle">
      <div class="one">
        <a href="https://arxiv.org/abs/2310.17923"><img src='images/dynamic_grasping.png' width="240" height="215"  style="vertical-align:middle;margin:-40px 40px"></a>
        <!-- <embed src="images/GRACE_teaser.pdf" width="190" /> -->
      </div>
    </td>
    <td style="padding:60px;width:75%;vertical-align:middle">
      <a href="https://arxiv.org/abs/2310.17923">
        <papertitle>Multi-fingered Dynamic Grasping for Unknown Objects</papertitle>
      </a>
      <br>
            <a href="https://srl.cit.tum.de/members/burkhary">Yannick Burkhardt</a>,
            <a href="https://www.ce.cit.tum.de/air/people/qian-feng-msc/">Qian Feng</a>,
            <strong>Jianxiang Feng</strong>,
            <a href="https://scholar.google.de/citations?user=_dL0__MAAAAJ&hl=en">Karan Sharma</a>,
            <a href="https://dblp.org/pid/63/8729.html">Zhaopeng Chen</a>,
            <a href="https://www.ce.cit.tum.de/air/people/prof-dr-ing-habil-alois-knoll/"> Alois Knoll</a>,
      <br>
IEEE-RAS International Conference on Humanoid Robots (Humanoids), 2024.
      <br>
      <a href="https://arxiv.org/abs/2310.17923">arxiv</a> /
      <a href="https://youtu.be/b87zGNoKELg">video</a>
          <p></p>
          <p>
          A dynamic grasping framework for unknown objects in this work, which uses a five-fingered hand with visual servo control and can compensate for external disturbances.
          </p>
  </tr>
<! ------------------------------------------------------- End of Dynamic Grasping ----------------------------------------------------------------- -->

<! ------------------------------------------------------- KnowLoop ----------------------------------------------------------------- -->
  <tr bgcolor="#ffffd0">
      <td style="padding:00px;width:25%;vertical-align:middle">
        <div class="one">
          <a href="https://arxiv.org/abs/2406.00430"><img src='images/icra24ws.png' width="210" height="225"  style="vertical-align:middle;margin:-40px 50px"></a>
          <!-- <embed src="images/GRACE_teaser.pdf" width="190" /> -->
        </div>
      </td>
      <td style="padding:60px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2406.00430">
          <papertitle>Evaluating Uncertainty-based Failure Detection for Closed-Loop LLM Planners</papertitle>
        </a>
        <br>
            Zhi Zheng*,
            <a href="https://www.ce.cit.tum.de/air/people/qian-feng-msc/">Qian Feng</a>,
            Hang Li,
            <a href="https://www.ce.cit.tum.de/air/people/prof-dr-ing-habil-alois-knoll/"> Alois Knoll</a>,
            <strong>Jianxiang Feng</strong>*,
        <br>
        Workshop on Back to the Future: Robot Learning Going Probabilistic, IEEE International Conference on Robotics and Automation (ICRA) 2024.
        <br>
        <a href="https://sites.google.com/view/konwloop/home">webiste</a>
        /   
        <a href="https://drive.google.com/file/d/1F2QZ2kfV2EOUCtwg3rzNU_kQyfMN9jEu/view?usp=sharing">poster</a>
        /
        <a href="https://arxiv.org/abs/2406.00430">arxiv</a>
        / 
          <a href="data/icra_ws_2024.bib">bibtex</a>
          <p></p>
          <p>
          An initial attempt to investigate the effects of several uncertainty metrics on an VLM failure detector within a high-level LLM task planner.
          </p>
  </tr>
<! ----------------------------------------------------------------------------------------------------------------------------- -->	
        
	
<! ------------------------------------------------------- SAM ----------------------------------------------------------------- -->
  <tr >
      <td style="padding:0px;width:25%;vertical-align:middle">
        <div class="one">
          <!-- <div class="two" id='FR_SAM_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/bal_edan.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div> -->
          <a href="https://elib.dlr.de/194517/1/Vol3_10.pdf"><img src='images/FR2023.png' width="275"  height="180" style="vertical-align:middle;margin:-20px 20px"></a>
        </div>
      </td>
    <td style="padding:60px;width:75%;vertical-align:middle">
      <a href="https://elib.dlr.de/194517/1/Vol3_10.pdf">
        <papertitle>Virtual Reality via Object Pose Estimation and Active Learning: Realizing Telepresence Robots with Aerial Manipulation Capabilities</papertitle>
      </a>
      <br>
      <a href="https://rmc.dlr.de/rm/en/staff/jongseok.lee/">Jongseok Lee</a>, <a href="https://rmc.dlr.de/rm/en/staff/ribin.balachandran/">Ribin Radhakrishna Balachandran</a> , <a href="https://de.linkedin.com/in/konstantin-kondak-02265526">Konstantin Kondak</a>, <a href="https://scholar.google.com.br/citations?user=HZlxXRYAAAAJ&hl=en">Andre Coelho</a>, <a href="https://rmc.dlr.de/rm/de/staff/marco.destefano/">Marco De Stefano</a>, <a href="https://www.hummat.com/">Matthias Humt</a>, <strong>Jianxiang Feng</strong>, <a href="https://h2t.anthropomatik.kit.edu/21_2372.php">Tamim Asfour</a>,   <a href="https://scholar.google.com/citations?user=SuOUxjUAAAAJ&hl=en">Rudolph Triebel</a>,
      <br>
      IEEE Transactions on Field Robotics, 2024.
      <br>
      Field Robotics, 2023.
      <br>
      <a href="https://www.youtube.com/watch?v=JRnPIARW8xY&ab_channel=DLRRM">video</a>
      /
      <a href="data/fr2023.bib">bibtex</a>
      <p></p>
      <p>
        A novel teleoperation system for advancing aerial manipulation in dynamic and unstructured environments based on pose estimation pipelines for the industrial objects of
        both known and unknown geometries and an active learning pipeline.
      </p>
    </td>
  </tr>
<! ------------------------------------------------------------------------------------------------------------------------ -->	

<! ------------------------------------------------------- NF4OOD ----------------------------------------------------------------- -->
  <tr bgcolor="#ffffd0">
      <td style="padding:0px;width:25%;vertical-align:middle">
        <div class="one">
          <a href="https://openreview.net/forum?id=BzjLaVvr955"><img src='images/corl23.png' width="290" height="200" style="vertical-align:middle;margin:-20px 10px"></a>
          <!-- <embed src="images/GRACE_teaser.pdf" width="190" /> -->
        </div>
      </td>
      <td style="padding:60px;width:75%;vertical-align:middle">
        <a href="https://openreview.net/forum?id=BzjLaVvr955">
          <papertitle>Topology-Matching Normalizing Flows for Out-of-Distribution Detection in Robot Learning</papertitle>
        </a>
        <br>
            <strong>Jianxiang Feng</strong>,
      <a href="https://rmc.dlr.de/rm/en/staff/jongseok.lee/">Jongseok Lee</a>,
            <a href="https://www.cs.cit.tum.de/en/daml/team/simon-geisler/">Simon Geisler</a>,
      <a href="https://www.cs.cit.tum.de/en/daml/team/damlguennemann/">Stephan Günnemann</a>,
            <a href="https://scholar.google.com/citations?user=SuOUxjUAAAAJ&hl=en">Rudolph Triebel</a>,
        <br>
        7th Conference on Robot Learning (CoRL) 2023.
        <br>
        <a href="https://github.com/DLR-RM/TMNF">code</a>
  /
        <a href="https://sites.google.com/view/tmnf-corl23/home">webiste</a>
        /   
        <a href="data/corl23_poster.pdf">poster</a>
        /
        <a href="data/corl23_NF_spotlight_1min_v2.mp4">spotlight presentation</a>
        /
        <!-- <a href="images/bal_edan.mp4">video</a>
        / -->
          <a href="data/corl23.bib">bibtex</a>
          <p></p>
          <p>
          A novel way to equip NFs with efficient but flexible base distributions to overcome the topological constraint for OOD detection in robot learning.
          </p>
  </tr>
<! ----------------------------------------------------------------------------------------------------------------------------- -->	
	 
<! ------------------------------------------------------- NF RASP ----------------------------------------------------------------- -->
        <tr>
          <td style="padding:0px;width:25%;vertical-align:middle">
            <div class="one">
              <!-- <img src='GRACE_teaser.pdf' width="160" height="2100px" > -->
              <a href="https://openreview.net/forum?id=Zw9rQMDl8m&referrer=[the%20profile%20of%20Jianxiang%20Feng]"><img src='images/NFs_ASP_teaser.png' width="290" height="170" style="vertical-align:middle;margin:-20px 0px"></a>
              <!-- <embed src="images/NFs_ASP_teaser.pdf" width="190" /> -->
            </div>
          </td>
          <td style="padding:60px;width:75%;vertical-align:middle">
            <a href="https://openreview.net/forum?id=Zw9rQMDl8m&referrer=[the%20profile%20of%20Jianxiang%20Feng]">
              <papertitle>Density-based Feasibility Learning with
                Normalizing Flows for Introspective Robotic
                Assembly</papertitle>
            </a>
            <br>
            <strong>Jianxiang Feng</strong>,
            <a href="https://aim-lab.io/author/matan-atad/"> Matan Atad </a>,
            <a href="https://scholar.google.com.uy/citations?user=NeZae4kAAAAJ&hl=es">Ismael Rodríguez</a>,
            <a href="https://rmc.dlr.de/rm/de/staff/maximilian.durner/">Maximilian Durner</a>,
      <a href="https://www.cs.cit.tum.de/en/daml/team/damlguennemann/">Stephan Günnemann</a>,
            <a href="https://scholar.google.com/citations?user=SuOUxjUAAAAJ&hl=en">Rudolph Triebel</a>,
            <br>
            Workshop on Robotics and AI: The Future of Industrial Assembly Tasks,
            Robotics: Science and Systems (RSS-WS) 2023.
            <br>
            <a href="https://github.com/DLR-RM/GRACE">code</a>
            /
            <a href="https://sites.google.com/view/nfasp/home">website</a> 
            /
            <a href="https://arxiv.org/abs/2307.01317">arxiv</a>
            /
            <a href="data/rss_ws_nf4rasp_v2.mp4">presentation video</a>
            /
            <a href="data/rss_ws_nf4rasp_v2.pdf">slides</a>
            /
            <a href="data/rss_ws_2023.bib">bibtex</a>
            <p></p>
            <p>
              A density-based method with normalizing flows for feasibility learning in Robotic Assembly based on only feasible examples.
            </p>
        </tr>
<! ----------------------------------------------------------------------------------------------------------------------------- -->	
      
<! ------------------------------------------------------- RASP ----------------------------------------------------------------- -->
<tr bgcolor="#ffffd0">
    <td style="padding:0px;width:25%;vertical-align:middle">
      <div class="one">
        <a href="https://arxiv.org/pdf/2303.10135.pdf"><img src='images/GRACE_teaser1.png' width="230" height="225"  style="vertical-align:middle;margin:-40px 40px"></a>
        <!-- <embed src="images/GRACE_teaser.pdf" width="190" /> -->
      </div>
    </td>
    <td style="padding:60px;width:75%;vertical-align:middle">
      <a href="https://ieeexplore.ieee.org/document/10342352">
        <papertitle>Efficient and Feasible Robotic Assembly Sequence Planning via Graph Representation Learning</papertitle>
      </a>
      <br>
      <strong>Jianxiang Feng*</strong>,
      <a href="https://aim-lab.io/author/matan-atad/"> Matan Atad </a>*,
      <a href="https://scholar.google.com.uy/citations?user=NeZae4kAAAAJ&hl=es">Ismael Rodríguez</a>,
      <a href="https://rmc.dlr.de/rm/de/staff/maximilian.durner/">Maximilian Durner</a>,
      <a href="https://scholar.google.com/citations?user=SuOUxjUAAAAJ&hl=en">Rudolph Triebel</a>,
      <br>
      IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2023.
      <br>
      <a href="https://github.com/DLR-RM/GRACE">code</a>
      /
      <a href="https://sites.google.com/view/asp-grace/home">website</a> 
      /
      <a href="https://arxiv.org/pdf/2303.10135">arXiv</a>
      /
      <a href="data/iros2023.bib">bibtex</a>
      <p></p>
      <p>
      A holistic graphical approach including a graph representation for product assemblies and a policy architecture, Graph Assembly Processing Network, dubbed GRACE to predict assembly sequences in a step-by-step manner.
      </p>
  </tr>
<! ----------------------------------------------------------------------------------------------------------------------------- -->	


<! ------------------------------------------------------- Survey ----------------------------------------------------------------- -->
    <tr >
      <td style="padding:0px;width:25%;vertical-align:middle">
        <div class="one">
          <a href="https://arxiv.org/abs/2107.03342"><img src='images/survey.png' width="270" height="170" style="vertical-align:middle;margin:-20px 30px"></a>
          <!-- <embed src="images/survey1.pdf" width="190" /> -->
        </div>
      </td>
      <td style="padding:60px;width:75%;vertical-align:middle">
        <a href="https://link.springer.com/article/10.1007/s10462-023-10562-9">
          <papertitle>A Survey of Uncertainty in Deep Neural Networks </papertitle>
        </a>
        <br>
        <a href="https://scholar.google.com/citations?user=JSDN9rsAAAAJ&hl=en">Jakob Gawlikowski</a>,
        Cedrique Rovile Njieutcheu Tassi, 
        Mohsin Ali, 
        <a href="https://rmc.dlr.de/rm/en/staff/jongseok.lee/">Jongseok Lee</a>,
        <a href="https://www.hummat.com/">Matthias Humt</a>,
        <strong>Jianxiang Feng</strong>,
        Anna Kruspe, 
        <a href="https://scholar.google.com/citations?user=SuOUxjUAAAAJ&hl=en">Rudolph Triebel</a>,
        Peter Jung, 
        Ribana Roscher, 
        Muhammad Shahzad, 
        Wen Yang, 
        Richard Bamler, 
        <a href="https://www.professoren.tum.de/en/zhu-xiaoxiang"> Xiaoxiang Zhu</a>
        <br> 
  Artificial Intelligence Review 2023.
        <br>
          <a href="https://arxiv.org/abs/2107.03342">arXiv</a>
        /
        <a href="data/survey.bib">bibtex</a> 
        <p></p>
        <p>
          A comprehensive overview of uncertainty estimation in neural networks, reviewing recent advances in the field, highlighting current challenges, and identifying potential research opportunities. 
        </p>
    </tr>
<! ----------------------------------------------------------------------------------------------------------------------------- -->	

<! ------------------------------------------------------- BAL EDAN ----------------------------------------------------------------- -->
   <tr bgcolor="#ffffd0">
      <td style="padding:0px;width:25%;vertical-align:middle">
        <div class="one">
          <!-- <div class="two" id='bal_edan_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/bal_edan.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div> -->
          <a href="https://arxiv.org/pdf/2109.11547"><img src='images/iros22_bal.png' width="240" height="240" style="vertical-align:middle;margin:-40px 40px"></a>
        </div>
      </td>
      <td style="padding:60px;width:75%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/9982175">
          <papertitle>Bayesian Active Learning for Sim-to-Real Robotic Perception</papertitle>
        </a>
        <br>
        <strong>Jianxiang Feng</strong>,
        <a href="https://rmc.dlr.de/rm/en/staff/jongseok.lee/">Jongseok Lee</a>,
        <a href="https://rmc.dlr.de/rm/de/staff/maximilian.durner/">Maximilian Durner</a>,
        <a href="https://scholar.google.com/citations?user=SuOUxjUAAAAJ&hl=en">Rudolph Triebel</a>,
        <br>
        IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2022.
        <br>
        <a href="https://github.com/DLR-RM/BayesSim2Real">code</a>
        /
	<a href="https://sites.google.com/view/bal-sim2real/home">website</a>
	/
        <a href="https://arxiv.org/pdf/2109.11547">arXiv</a>
        /
        <a href="data/iros22_poster.pdf">poster</a>
        /
        <a href="data/iros2022_bayesSim2real_v2.pdf">slides</a>
        /
        <a href="data/IROS22_1546.mp4">presentation video</a>
        /
        <a href="data/bal_edan.mp4">demo video</a>
        <!-- <video width="560" height="315" playsinline="" muted="" autoplay="" loop=""><source src="images/bal_edan.mp4" type="video/mp4"></video></a> -->
        /
        <a href="data/bal2022.bib">bibtex</a>
        <p></p>
        <p>
          An active learning pipeline to reduce annotation efforts of real data within a Sim-to-Real scenario based on deep Bayesian Neural Networks.
        </p>
    </tr>
<! ----------------------------------------------------------------------------------------------------------------------------- -->	      
	      
<! ------------------------------------------------------- CoRL21 ----------------------------------------------------------------- -->
   <tr bgcolor="#ffffd0">
      <td style="padding:0px;width:25%;vertical-align:middle">
        <div class="one">
          <!-- <embed src="images/corl21.pdf" width="190" /> -->
          <a href="https://proceedings.mlr.press/v164/lee22c.html"><img src='images/corl21.png' width="285" height="180" style="vertical-align:middle;margin:-20px 10px"></a>
        </div>
      </td>
      <td style="padding:60px;width:75%;vertical-align:middle">
        <a href="https://proceedings.mlr.press/v164/lee22c.html">
          <papertitle>Trust Your Robots! Predictive Uncertainty Estimation of Neural Networks with Sparse Gaussian Processes</papertitle>
        </a>
        <br>
        <a href="https://rmc.dlr.de/rm/en/staff/jongseok.lee/">Jongseok Lee</a>,
        <strong>Jianxiang Feng</strong>,
        <a href="https://www.hummat.com/">Matthias Humt</a>,
        <a href="https://rmc.dlr.de/rm/de/staff/marcus.mueller/">Marcus G. Muller</a>,
        <a href="https://scholar.google.com/citations?user=SuOUxjUAAAAJ&hl=en">Rudolph Triebel</a>,
        <br>
        5th Conference on Robot Learning (CoRL) 2021.
        <br>
        <a href="https://github.com/DLR-RM/moegplib">code</a>
        /
        <a href="https://www.youtube.com/watch?v=vu2TnDEqDRk&t=37s&ab_channel=DLRRM">video</a>
        /
        <!-- <a href="https://arxiv.org/pdf/2109.12869">arXiv</a>
        / -->
        <a href="data/pmlr-v164-lee22c.bib">bibtex</a>
        <p></p>
        <p>
        A probabilistic framework to obtain both reliable and fast uncertainty estimates for predictions with Deep Neural Networks (DNNs) based on Sparse Gaussian Processes. 
        </p>
    </tr>
<! ----------------------------------------------------------------------------------------------------------------------------- -->	


<! ------------------------------------------------------- ICML2020 ----------------------------------------------------------------- -->
   <tr bgcolor="#ffffd0">
      <td style="padding:0px;width:25%;vertical-align:middle">
        <div class="one">
            <!-- <embed src="images/icml20.pdf" width="190" /> -->
            <a href="http://proceedings.mlr.press/v119/lee20b.html"><img src='images/icml20.png' width="225" height="230"  style="vertical-align:middle;margin:-50px 50px"></a>
        </div>
      </td>
      <td style="padding:60px;width:75%;vertical-align:middle">
        <a href="http://proceedings.mlr.press/v119/lee20b.html">
          <papertitle>Estimating Model Uncertainty of Neural Networks in Sparse Information Form  </papertitle>
        </a>
        <br>
        <a href="https://rmc.dlr.de/rm/en/staff/jongseok.lee/">Jongseok Lee</a>,
        <a href="https://www.hummat.com/">Matthias Humt</a>,
        <strong>Jianxiang Feng</strong>,
        <a href="https://scholar.google.com/citations?user=SuOUxjUAAAAJ&hl=en">Rudolph Triebel</a>,
        <br>
        International Conference on Machine Learning (ICML) 2020.
        <br>
        <a href="https://github.com/DLR-RM/curvature">code</a>
        /
        <!-- <a href="https://www.youtube.com/watch?v=vu2TnDEqDRk&t=37s&ab_channel=DLRRM">video</a>
        / -->
        <!-- <a href="https://arxiv.org/pdf/2109.12869">arXiv</a>
        / -->
        <a href="data/pmlr-v119-lee20b.bib">bibtex</a>
        <p></p>
        <p>
         A sparse representation of model uncertainty for Deep Neural Networks (DNNs) where the parameter posterior is approximated with an inverse formulation of the Multivariate Normal Distribution (MND), also known as the information form. 
        </p>
    </tr>
<! ----------------------------------------------------------------------------------------------------------------------------- -->	
	

<! ------------------------------------------------------- ISRR ----------------------------------------------------------------- -->
   <tr bgcolor="#ffffd0">
      <td style="padding:0px;width:25%;vertical-align:middle">
        <div class="one">
          <!-- <embed src="images/isrr_teaser.pdf" width="190" /> -->
          <a href="https://arxiv.org/pdf/2109.12869"><img src='images/isrr_teaser.png' width="310" height="160"  style="vertical-align:middle;margin:10px 5px"></a>
        </div>
      </td>
      <td style="padding:60px;width:75%;vertical-align:middle">
        <a href="https://link.springer.com/chapter/10.1007/978-3-030-95459-8_40">
          <papertitle>Introspective Robot Perception using Smoothed Predictions from Bayesian Neural Networks</papertitle>
        </a>
        <br>
        <strong>Jianxiang Feng</strong>,
        <a href="https://rmc.dlr.de/rm/de/staff/maximilian.durner/">Maximilian Durner</a>,
        <a href="https://scholar.google.com/citations?user=VGBlCk4AAAAJ&hl=en">Zoltán-Csaba Márton</a>,
        <a href="https://ai.uni-bremen.de/team/ferenc_balint-benczedi">Ferenc Bálint-Benczédi</a>,
        <a href="https://scholar.google.com/citations?user=SuOUxjUAAAAJ&hl=en">Rudolph Triebel</a>,
        <br>
        The International Symposium of Robotics Research (ISRR) 2019.
        <br>
        <a href="https://sites.google.com/view/bnnperception/home">website</a>
        /
	<!--<a href="https://www.youtube.com/watch?v=EoER_5vYZsU&t=3s&ab_channel=DLRRM">video</a>
        / -->
        <a href="https://arxiv.org/pdf/2109.12869">arXiv</a>
        /
        <a href="data/isrr19.bib">bibtex</a>
        <p></p>
        <p>
        A method for adaptive image classification based on fusing uncertainty estimates from Bayesian Neural Networks as unary potentials within a Conditional Random Field (CRF).
        </p>
    </tr>
<! ----------------------------------------------------------------------------------------------------------------------------- -->	
	


</table>

<! ------------------------------------------------------- End of Publications ----------------------------------------------------------------- -->

				
	<! ------------------------------------------------------- Academic Services ---------------------------------------------------------- -->	
  
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <br>
              <br>
              <br>
              <h1 style="font-size: 30px">Academic Services</h1>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">

              Conference/Journal Reviewer: 
              <ul>
                <li>International Conference on Learning Representations (ICLR) 2025</li>
                <li>Conference on Robot Learning (CoRL) 2022-2025</li>
                <li>IEEE International Conference on Robotics and Automation (ICRA) 2020&2022&2025</li>
                <li>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2020&2022&2025</li>
                <li>European Conference on Artificial Intelligence (ECAI) 2020</li>
                <li>IEEE-RAS International Conference on Humanoid Robots 2024</li>
                <li>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</li>
                <li>IEEE Robotics and Automation Letters (RAL)</li>
              </ul>  
              <br>
              <br>
              Workshop Organizer: 
              <ul>
                <li>ICRA2024 Workshop on <a href="https://probabilisticrobotics.github.io/">Back to the Future: Robot Learning Going Probabilistic</a></li> 
                <li>IROS2022 Workshop on <a href="https://probabilisticrobotics.github.io/2022/">Probabilistic Robotics in the age of Deep Learning</a></li> 
              </ul>  
              <br>

        <!-- <div class="list-item highlight" data-category="highlight">
                <p class="date">2024</p> &nbsp;&nbsp;&nbsp; Reviewer, CoRL.
              </div>	    

        <div class="list-item highlight" data-category="highlight">
                <p class="date">05.2024</p> &nbsp;&nbsp;&nbsp; Co-Organizer, ICRA Workshop on <a href="https://probabilisticrobotics.github.io/">Back to the Future: Robot Learning Going Probabilistic</a>.
              </div>
          
        <div class="list-item highlight" data-category="highlight">
                <p class="date">2023</p> &nbsp;&nbsp;&nbsp; Reviewer, CoRL.
              </div>	    

              <div class="list-item highlight" data-category="highlight">
              <p class="date">10.2022</p> &nbsp;&nbsp;&nbsp; Co-Organizer, IROS Workshop on <a href="https://probabilisticrobotics.github.io/2022/">Probabilistic Robotics in the age of Deep Learning</a>.
              </div>
                
              <div class="list-item highlight" data-category="highlight">
                <p class="date">2022</p> &nbsp;&nbsp;&nbsp; Reviewer, ICRA, CoRL, IROS.
              </div>
            
              <div class="list-item highlight" data-category="highlight">
                <p class="date">2020</p> &nbsp;&nbsp;&nbsp; Reviewer, ICRA, IROS, ECAI.
              </div> -->
              
            
            </td>
          <!-- </tr> -->
      
	<! ------------------------------------------------------------------------------------------------------------------------ -->		
				
	<! ------------------------------------------------------- Events ---------------------------------------------------------- -->	
  
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <h1 style="font-size: 30px">Events</h1>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">

		    
	   <div class="list-item highlight" data-category="highlight">
		<p class="date">06.2023</p> &nbsp;&nbsp;&nbsp; Team player, the <a href=https://www.linkedin.com/feed/update/urn:li:activity:7091522148384550913/?origin=SHARED_BY_YOUR_NETWORK>aerial manipulation demo</a> for <a href="https://www.kuka.com/en-us/future-production/research-and-development/kuka-innovation-award/kuka-innovation-award-2023">kuka inovation award 2023</a> in <a href="https://automatica-munich.com/en/">Automatica</a> 2023 (<a href="https://www.linkedin.com/posts/kukaglobal_team-jarvis-from-the-merlin-labodratory-of-activity-7080170709481574400-7guO?utm_source=share&utm_medium=member_desktop">press</a>).
	   </div><div class="list-item highlight" data-category="highlight">
			
	    <div class="list-item highlight" data-category="highlight">
		 <p class="date">03.2023</p> &nbsp;&nbsp;&nbsp; Team player, the 1st place in the discipline assist robot race of <a href="https://cybathlon.ethz.ch/de/teams/edan">Cybathlon Challenge</a> as part of EDAN team (<a href="https://www.dlr.de/de/rm/aktuelles/nachrichten/2023/teilnahme-an-den-cybathlon-challenges-2023">press</a>, <a href="https://www.youtube.com/watch?v=EoER_5vYZsU&t=3s&ab_channel=DLRRM">video</a>)
            </div>
			
          <div class="list-item highlight" data-category="highlight">
              <p class="date">06.2022</p> &nbsp;&nbsp;&nbsp; Team player, <a href="https://www.dlr.de/rm/desktopdefault.aspx/tabid-11670/#gallery/28208">EDAN</a> demo at Automatica Exhibition 2022 (<a href="https://www.dlr.de/de/rm/aktuelles/nachrichten/2022/automatica-2022">press</a>).
            </div>
            
            </td>
          <!-- </tr> -->
      
	<! ------------------------------------------------------------------------------------------------------------------------ -->		
				
	<! ------------------------------------------------------- Misc ---------------------------------------------------------- -->	


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <h1 style="font-size: 30px">Mentorship</h1>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
		    
	<div class="list-item highlight" data-category="highlight">
            <p class="date">08.2023</p> &nbsp;&nbsp;&nbsp; Master Thesis Supervision at TUM: "Improving Sample Selection in Active Learning Using Graph Neural Networks" by Zhoumin Zhao, co-supervision with <a href="https://www.cs.cit.tum.de/en/daml/team/simon-geisler/">Simon Geisler</a>.
           </div>  
           <br>

		    
	  <div class="list-item highlight" data-category="highlight">
            <p class="date">06.2023</p> &nbsp;&nbsp;&nbsp; Research Internship at DLR: "Automating Scene Graph Data Generation for Task and Motion Planning via Blenderproc" by Juan Diego Plaza Gomez, co-supervision with <a href="https://rmc.dlr.de/rm/de/staff/samuel.bustamante/">Samuel Bustamante</a> and <a href="https://scholar.google.de/citations?user=kduGd8wAAAAJ&hl=de">Dominik Winkelbauer</a>.
           </div>  
           <br>
		    
		
            <div class="list-item highlight" data-category="highlight">
            <p class="date">03.2023</p> &nbsp;&nbsp;&nbsp; Master Thesis Supervision at TUM: <a href="https://elib.dlr.de/194212/">"Graph Neural Networks for Knowledge Transfer in Robotic Assembly Sequence Planning"</a> by Matan Atad, co-supervision with <a href="https://rmc.dlr.de/rm/de/staff/maximilian.durner/">Maximilian Durner</a>, <a href="https://rmc.dlr.de/rm/en/staff/ismael.rodriguezbrena/index.html">Ismael Rodriguezbrena</a>.
           </div>  
           <br>
          
            <div class="list-item highlight" data-category="highlight">
              <p class="date">11.2022</p> &nbsp;&nbsp;&nbsp; Master Thesis Supervision at TUM: <a href="https://elib.dlr.de/194212/">"Scene Graph Generation from Visual perception for Task and Motion Planning"</a> by Mohit Kumar, co-supervision with <a href="https://rmc.dlr.de/rm/en/staff/samuel.bustamante/">Samuel Bustamante</a>.
            </div>  
            <br>
            
            </td>
          <!-- </tr> -->
		  
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
        <tr>
          <td>
            <h1 style="font-size: 30px">Fun Moments and Hobbies</h1>
          </td>
        </tr>
      </tbody></table>
      <table width="100%" align="center" border="0" cellpadding="20"><tbody>
        <tr>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="https://www.instagram.com/luffy_fjx/">Instagram</a>
      
	<! ------------------------------------------------------------------------------------------------------------------------ -->				
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://github.com/jonbarron/jonbarron_website">Link to the template</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

  <script>
        function toggle_bio() {
        var x = document.getElementById("more-bio");
        if (x.style.display === "none") {
          x.style.display = "block";
        } else {
          x.style.display = "none";
        }
      }
  </script>
</body>

</html>
