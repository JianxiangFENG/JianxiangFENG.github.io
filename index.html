<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jianxiang Feng</title>
  
  <meta name="author" content="Jianxiang Feng">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="images/favicon.ico">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jian-xiang Feng / 冯健祥</name>
              </p>
              <p>Hi, there! I am a final-year PhD student at <a href="https://www.tum.de/en">Technical University of Munich (TUM)</a> and the <a href="https://www.dlr.de/rm/en/desktopdefault.aspx/tabid-8017/">Institute of Robotic and Mechtronic</a> (RM), <a href="https://www.dlr.de/en/">German Aerospace Center</a> (DLR). 
		      I am advised by Prof. <a href="https://rmc.dlr.de/rm/de/staff/rudolph.triebel/"> Rudolph Triebel</a> (DLR&KIT) and affiliated with <a href="https://www.mu-ds.de/"> Munich School of Data Science</a>, where I worked with Prof. <a href="https://www.cs.cit.tum.de/en/daml/team/damlguennemann/">Stephan Günnemann</a>. 
		      Recently, I have been working on applying my previous research to grasping and LLMs for Robotics as a senior research scientist at <a href="https://www.agile-robots.com/en/"> Agile Robots SE</a>.
              </p>
              <p>
                In general, my research interests reside in the intersection of robotics and machine learning with a primary focus on the <strong>trustworthy</strong> and <strong>adaptable</strong> learning ability of a robot in an <strong>open-world</strong> environment. In particular, aiming to equip a robot with <strong>introspective capabilities</strong> i.e., reliable confidence estimates and an awareness of the internal state of the system scuh as limitation of its knowledge and skills, I am intereseted in levaraging <strong>probabilistic Machine Learning</strong> methods such as Bayesian Neural Networks, Probabilistic Graphicial Models, flow-based Deep Generative Models, on 
		      <br><br>
		      &nbsp;&nbsp;&nbsp;&nbsp; - how to provide such introspection quantitatively: Uncertainty Estimation, Out-Of-Distribution detection, etc. 
		      <br><br>
		      &nbsp;&nbsp;&nbsp;&nbsp; - how to exploit this capability for learning-enabled components on robots: perception, assembly planning, active learning, grasping, etc. 
		      <br><br>
		      I believe that this can lead to more explainable and ultimately safer autonomous robot systems.
              </p>
	      <p> P.S. The pronunciation of my first name "Jianxiang" is quite close to "Jensen" (yep, the well known "Jensen inequality" often used in ELBO derivation) but with a different ending :P. 
	      </p>
		    
              <div id="more-bio" style="display: None">
                <p>Jianxiang Feng received his bachelor degree from Beijing University of Posts and Telecommunication (BUPT) in 2015 and his master degree from Technical University of Munich (TUM) in 2019. Since August 2019, he pursued his PhD at TUM and the Institute of Robotics and Mechatronics (RM), the German Aerospace Center (DLR). His research interests reside in the intersection of robotics and machine learning.</p>
              </div>
              <p style="text-align:center">
                <a href="javascript:toggle_bio()">Short Bio</a>&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://drive.google.com/file/d/1GuVC6gXGBkzs1LMAGm0i1AsWvT9m4FvT/view?usp=sharing">CV</a> &nbsp;&nbsp;&nbsp;&nbsp;
<!-- 			/<a href="https://drive.google.com/file/d/178MR_pM2WFGkKol1vmE6DO13ew30V__z/view?usp=sharing">Research</a></a>&nbsp;&nbsp;&nbsp;&nbsp; -->
                <a href="https://github.com/JianxiangFENG">Github</a>&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://scholar.google.com/citations?user=b-5CscIAAAAJ&hl=en">G. Scholar</a> &nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://www.linkedin.com/in/jianxiangfeng/">LinkedIn</a>&nbsp;&nbsp;&nbsp;&nbsp;
<!--                 <a href="https://twitter.com/fengjianxiang?lang=en">Twitter</a>&nbsp;&nbsp;&nbsp;&nbsp; -->
                <br><br>
                jianxiang.feng at tum dot de
                <br><br>
              </p>
		
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/profile-half.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <h1 style="font-size: 30px">News</h1>
            <p>
              <div class="list-item highlight" data-category="highlight">
	      <p class="date">04.2024</p> &nbsp;&nbsp;&nbsp; Our recent work <a href="https://sites.google.com/view/konwloop/home">KnowLoop</a>, an uncertainty-based VLM failure detector for closed-loop LLM-based planners, got accepted to the ICRA 2024 <a href="https://probabilisticrobotics.github.io/">Workshop</a>. </div><div class="list-item highlight" data-category="highlight">
		    
              <div class="list-item highlight" data-category="highlight">
	      <p class="date">01.2024</p> &nbsp;&nbsp;&nbsp; Our ICRA 2024 <a href="https://probabilisticrobotics.github.io/">Workshop Proposal</a> titled "Back to the Future: Robot Learning Going Probabilistic" got accepted, thrilled to co-organize it! Stay tuned!</div><div class="list-item highlight" data-category="highlight">
		    
              <div class="list-item highlight" data-category="highlight">
	      <p class="date">08.2023</p> &nbsp;&nbsp;&nbsp; A <a href="https://openreview.net/forum?id=BzjLaVvr955&referrer=%5Bthe%20profile%20of%20Jianxiang%20Feng%5D(%2Fprofile%3Fid%3D~Jianxiang_Feng1">paper</a> on Normalizing Flows for Out-of-Distribution Detection got accepted in CoRL2023!
              </div><div class="list-item highlight" data-category="highlight">
		      
              <p class="date">07.2023</p> &nbsp;&nbsp;&nbsp; A <a href="https://arxiv.org/pdf/2303.10135.pdf">paper</a> on Graph Neural Networs and Assembly Seqeunce Planning got accepted in IROS2023.
              </div><div class="list-item highlight" data-category="highlight">

                <div class="list-item highlight" data-category="highlight">
                <p class="date">06.2023</p> &nbsp;&nbsp;&nbsp; Our <a href=https://www.linkedin.com/feed/update/urn:li:activity:7091522148384550913/?origin=SHARED_BY_YOUR_NETWORK>aerial manipulation demo</a> for <a href="https://www.kuka.com/en-us/future-production/research-and-development/kuka-innovation-award/kuka-innovation-award-2023">kuka inovation award 2023</a> gained immense popularity in <a href="https://automatica-munich.com/en/">Automatica</a> 2023 (<a href="https://www.linkedin.com/posts/kukaglobal_team-jarvis-from-the-merlin-labodratory-of-activity-7080170709481574400-7guO?utm_source=share&utm_medium=member_desktop">press</a>).
                </div><div class="list-item highlight" data-category="highlight">

              <div class="list-item highlight" data-category="highlight">
              <p class="date">06.2023</p> &nbsp;&nbsp;&nbsp; A <a href="https://arxiv.org/abs/2307.01317">workshop paper</a> on <a href="https://sites.google.com/nvidia.com/industrial-assembly">Robotics and AI: The Future of Industrial Assembly Tasks</a> at RSS 2023 got accepted.
              </div><div class="list-item highlight" data-category="highlight">

	    <div class="list-item highlight" data-category="highlight">
		 <p class="date">03.2023</p> &nbsp;&nbsp;&nbsp; Won the 1st place in the discipline assist robot race of <a href="https://cybathlon.ethz.ch/de/teams/edan">Cybathlon Challenge</a> as part of EDAN team (<a href="https://www.dlr.de/rm/en/desktopdefault.aspx/tabid-3755/17612_read-81519/year-2023/">press</a>, <a href="https://www.youtube.com/watch?v=EoER_5vYZsU&t=3s&ab_channel=DLRRM">video</a>)
            </div>
            </p>
          </td>
        </tr>
      </tbody></table>


      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h1 style="font-size: 30px">Videos</h1>
            <!-- <p>
              <h4> &emsp;&emsp;&emsp;&emsp; <a href="https://cybathlon.ethz.ch/de/teams/edan">Cybathlon Challenges </a> (assistant robot race) 
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;  Bayesian Active Learning Demo on <a href="https://www.dlr.de/rm/en/desktopdefault.aspx/tabid-11670/20388_read-47709/">EDAN</a>
                &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Bayesian Active Learning Demo on <a href="https://www.youtube.com/watch?v=ZoNNjQfUdJw&ab_channel=DLRRM">SAM</a></h4>
              </p> -->
            <p></p>
            <p></p>
            <div class="list-item highlight previews" data-category="highlight">
              <a href="https://www.youtube.com/watch?v=EoER_5vYZsU&t=3s&ab_channel=DLRRM"><iframe width="500" height="350" src="https://www.youtube.com/embed/EoER_5vYZsU?&autoplay=1&loop=1&playlist=EoER_5vYZsU&mute=1&start=100&showinfo=0" title="Cybathlon" frameborder="0" allow="accelerometer; allow='autoplay'; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
              </a>  
              <!-- <h4> CYBATHLON Challenges (assistant robot race)</h4> -->
              
              <!-- <h4> Bayesian Active Learning Demo on an assitive robot EDAN</h4> -->
              <a href="https://arxiv.org/pdf/2109.11547"><iframe width="500" height="350" src="https://www.youtube.com/embed/kfWNEBbvuSE?&autoplay=1&loop=1&playlist=kfWNEBbvuSE&mute=1&start=31&showinfo=0" title="Edan" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
	            </a> 
              
             <!--  <h4> Bayesian Active Learning Demo on an assitive robot EDAN</h4>
              <a href="https://arxiv.org/pdf/2109.11547"><video width="560" height="315" playsinline="" muted="" autoplay="" loop=""><source src="images/bal_edan.mp4" type="video/mp4"></video></a> -->
        
              </div>

              <div class="grid"></div>
              <div class="grid"></div>
              
              <div class="list-item highlight previews" data-category="highlight">
    
              <!-- <h4> Bayesian Active Learning Demo on aerial manipulation robot SAM</h4> -->
              <a href="https://www.youtube.com/watch?v=JRnPIARW8xY&ab_channel=DLRRM"><iframe width="500" height="350" src="https://www.youtube.com/embed/JRnPIARW8xY?&autoplay=1&loop=1&playlist=JRnPIARW8xY&mute=1&start=215&showinfo=0" title="SAM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
              </a>
              
              <!-- <h4> OOD detection Demo on aerial manipulation robnot SAM</h4> -->
              <a href="https://www.youtube.com/watch?v=L09z-AuRI0A"><iframe width="500" height="350" src="https://www.youtube.com/embed/L09z-AuRI0A?si=qcZgZu5TWW5P7FGT?&autoplay=1&loop=1&playlist=L09z-AuRI0A&mute=1&start=10&showinfo=0" title="ood_sam" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
              </a>
              </div>

              <div class="grid"></div>

            <!-- <div class="list-item highlight previews" data-category="highlight">
              <h4> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="https://cybathlon.ethz.ch/de/teams/edan">Cybathlon Challenges </a> (assistant robot race)</h4>
              <h4> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Bayesian Active Learning Demo on <a href="https://www.dlr.de/rm/en/desktopdefault.aspx/tabid-11670/20388_read-47709/">EDAN</h4></a>
              <h4> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Bayesian Active Learning Demo on <a href="https://www.youtube.com/watch?v=ZoNNjQfUdJw&ab_channel=DLRRM">SAM</a></h4>
            </div> -->
          <!-- </p> -->
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <!-- </tbody> -->
        <!-- </table> -->
        </td>
      </tr>

       
              
<! ------------------------------------------------------- Publications ----------------------------------------------------------------- -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <br>
            <br>
            <br>
            <br>
            <br>
            <h1 style="font-size: 30px">Selected Publications</h1>
            <br>
            <br>(*: equal contribution.)
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


	<! ------------------------------------------------------- NF4OOD ----------------------------------------------------------------- -->
        <tr  bgcolor="#ffffd0">
           <td style="padding:20px;width:25%;vertical-align:middle">
             <div class="one">
               <a href="https://openreview.net/forum?id=BzjLaVvr955"><img src='images/corl23.png' width="245" height="170"></a>
               <!-- <embed src="images/GRACE_teaser.pdf" width="190" /> -->
             </div>
           </td>
           <td style="padding:20px;width:75%;vertical-align:middle">
             <a href="https://openreview.net/forum?id=BzjLaVvr955">
               <papertitle>Topology-Matching Normalizing Flows for Out-of-Distribution Detection in Robot Learning</papertitle>
             </a>
             <br>
                  <strong>Jianxiang Feng</strong>,
        	  <a href="https://rmc.dlr.de/rm/en/staff/jongseok.lee/">Jongseok Lee</a>,
                  <a href="https://www.cs.cit.tum.de/en/daml/team/simon-geisler/">Simon Geisler</a>,
	          <a href="https://www.cs.cit.tum.de/en/daml/team/damlguennemann/">Stephan Günnemann</a>,
                  <a href="https://rmc.dlr.de/rm/de/staff/rudolph.triebel/">Rudolph Triebel</a>,
             <br>
             7th Conference on Robot Learning (CoRL) 2023.
             <br>
             <a href="https://github.com/DLR-RM/TMNF">code</a>
	     /
             <a href="https://sites.google.com/view/tmnf-corl23/home">webiste</a>
             /   
              <a href="data/corl23_poster.pdf">poster</a>
              /
              <a href="data/corl23_NF_spotlight_1min_v2.mp4">spotlight presentation</a>
              /
              <!-- <a href="images/bal_edan.mp4">video</a>
              / -->
                <a href="data/corl23.bib">bibtex</a>
                <p></p>
                <p>
                A novel way to equip NFs with efficient but flexible base distributions to overcome the topological constraint for OOD detection in robot learning.
                </p>
        </tr>
     <! ----------------------------------------------------------------------------------------------------------------------------- -->	
	      
<! ------------------------------------------------------- BAL EDAN ----------------------------------------------------------------- -->
<!--     <tr onmouseout="bal_edan_stop()" onmouseover="bal_edan_start()"  bgcolor="#ffffd0"> -->
   <tr  bgcolor="#ffffd0" >
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <!-- <div class="two" id='bal_edan_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/bal_edan.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div> -->
          <a href="https://arxiv.org/pdf/2109.11547"><img src='images/iros22_bal.png' width="230" height="210" style="vertical-align:middle;margin:-20px 10px"></a>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/9982175">
          <papertitle>Bayesian Active Learning for Sim-to-Real Robotic Perception</papertitle>
        </a>
        <br>
        <strong>Jianxiang Feng</strong>,
        <a href="https://rmc.dlr.de/rm/en/staff/jongseok.lee/">Jongseok Lee</a>,
        <a href="https://rmc.dlr.de/rm/de/staff/maximilian.durner/">Maximilian Durner</a>,
        <a href="https://rmc.dlr.de/rm/de/staff/rudolph.triebel/">Rudolph Triebel</a>,
        <br>
        IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2022.
        <br>
        <a href="https://github.com/DLR-RM/BayesSim2Real">code</a>
        /
	<a href="https://sites.google.com/view/bal-sim2real/home">website</a>
	/
        <a href="https://arxiv.org/pdf/2109.11547">arXiv</a>
        /
        <a href="data/iros22_poster.pdf">poster</a>
        /
        <a href="data/iros2022_bayesSim2real_v2.pdf">slides</a>
        /
        <a href="data/IROS22_1546.mp4">presentation video</a>
        /
        <a href="data/bal_edan.mp4">demo video</a>
        <!-- <video width="560" height="315" playsinline="" muted="" autoplay="" loop=""><source src="images/bal_edan.mp4" type="video/mp4"></video></a> -->
        /
        <a href="data/bal2022.bib">bibtex</a>
        <p></p>
        <p>
          An active learning pipeline to reduce annotation efforts of real data within a Sim-to-Real scenario based on deep Bayesian Neural Networks.
        </p>
    </tr>
<! ----------------------------------------------------------------------------------------------------------------------------- -->	      
	      
<! ------------------------------------------------------- CoRL21 ----------------------------------------------------------------- -->
<!--     <tr onmouseout="bal_edan_stop()" onmouseover="bal_edan_start()"  bgcolor="#ffffd0"> -->
   <tr  bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <!-- <embed src="images/corl21.pdf" width="190" /> -->
          <a href="https://proceedings.mlr.press/v164/lee22c.html"><img src='images/corl21.png' width="245" height="150"></a>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://proceedings.mlr.press/v164/lee22c.html">
          <papertitle>Trust Your Robots! Predictive Uncertainty Estimation of Neural Networks with Sparse Gaussian Processes</papertitle>
        </a>
        <br>
        <a href="https://rmc.dlr.de/rm/en/staff/jongseok.lee/">Jongseok Lee</a>,
        <strong>Jianxiang Feng</strong>,
        <a href="https://www.hummat.com/">Matthias Humt</a>,
        <a href="https://rmc.dlr.de/rm/de/staff/marcus.mueller/">Marcus G. Muller</a>,
        <a href="https://rmc.dlr.de/rm/de/staff/rudolph.triebel/">Rudolph Triebel</a>,
        <br>
        5th Conference on Robot Learning (CoRL) 2021.
        <br>
        <a href="https://github.com/DLR-RM/moegplib">code</a>
        /
        <a href="https://www.youtube.com/watch?v=vu2TnDEqDRk&t=37s&ab_channel=DLRRM">video</a>
        /
        <!-- <a href="https://arxiv.org/pdf/2109.12869">arXiv</a>
        / -->
        <a href="data/pmlr-v164-lee22c.bib">bibtex</a>
        <p></p>
        <p>
        A probabilistic framework to obtain both reliable and fast uncertainty estimates for predictions with Deep Neural Networks (DNNs) based on Sparse Gaussian Processes. 
        </p>
    </tr>
<! ----------------------------------------------------------------------------------------------------------------------------- -->	

<! ------------------------------------------------------- ISRR ----------------------------------------------------------------- -->
<!--     <tr onmouseout="bal_edan_stop()" onmouseover="bal_edan_start()"  bgcolor="#ffffd0"> -->
   <tr  bgcolor="#ffffd0" >
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <!-- <embed src="images/isrr_teaser.pdf" width="190" /> -->
          <a href="https://arxiv.org/pdf/2109.12869"><img src='images/isrr_teaser.png' width="250" height="120"  style="vertical-align:middle;margin:15px 0px"></a>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://link.springer.com/chapter/10.1007/978-3-030-95459-8_40">
          <papertitle>Introspective Robot Perception using Smoothed Predictions from Bayesian Neural Networks</papertitle>
        </a>
        <br>
        <strong>Jianxiang Feng*</strong>,
        <a href="https://rmc.dlr.de/rm/de/staff/maximilian.durner/">Maximilian Durner*</a>,
        <a href="https://scholar.google.com/citations?user=VGBlCk4AAAAJ&hl=en">Zoltán-Csaba Márton</a>,
        <a href="https://ai.uni-bremen.de/team/ferenc_balint-benczedi">Ferenc Bálint-Benczédi</a>,
        <a href="https://rmc.dlr.de/rm/de/staff/rudolph.triebel/">Rudolph Triebel</a>,
        <br>
        The International Symposium of Robotics Research (ISRR) 2019.
        <br>
        <a href="https://sites.google.com/view/bnnperception/home">website</a>
        /
	<!--<a href="https://www.youtube.com/watch?v=EoER_5vYZsU&t=3s&ab_channel=DLRRM">video</a>
        / -->
        <a href="https://arxiv.org/pdf/2109.12869">arXiv</a>
        /
        <a href="data/isrr19.bib">bibtex</a>
        <p></p>
        <p>
        A method for adaptive image classification based on fusing uncertainty estimates from Bayesian Neural Networks as unary potentials within a Conditional Random Field (CRF).
        </p>
    </tr>
<! ----------------------------------------------------------------------------------------------------------------------------- -->	
	
  </tbody></table>
<! ------------------------------------------------------- End of Selected Publication ----------------------------------------------------------------- -->


<! ------------------------------------------------------- Other Publications ----------------------------------------------------------------- -->
	        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
	          <tbody>
	            <tr>
	            <td style="padding:20px;width:100%;vertical-align:middle">
                <br>
                <br>
                <h1 style="font-size: 30px">Publications</h1>
                <br>
	            </td>
	          </tr>
	        </tbody>
	      </table>
	      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
 <! ------------------------------------------------------- KnowLoop ----------------------------------------------------------------- -->
        <tr >
           <td style="padding:20px;width:25%;vertical-align:middle">
             <div class="one">
               <a href="https://openreview.net/forum?id=9w1JnHG8Wn"><img src='images/icra24ws.png' width="215" height="220"  style="vertical-align:middle;margin:-40px 20px"></a>
               <!-- <embed src="images/GRACE_teaser.pdf" width="190" /> -->
             </div>
           </td>
           <td style="padding:20px;width:75%;vertical-align:middle">
             <a href="https://openreview.net/forum?id=9w1JnHG8Wn">
               <papertitle>Evaluating Uncertainty-based Failure Detection for Closed-Loop LLM Planners</papertitle>
             </a>
             <br>
                  Zhi Zheng*,
        	        <a href="https://www.ce.cit.tum.de/air/people/qian-feng-msc/">Qian Feng</a>,
                  Hang Li,
	                <a href="https://www.ce.cit.tum.de/air/people/prof-dr-ing-habil-alois-knoll/"> Alois Knoll</a>,
                  <strong>Jianxiang Feng</strong>*,
             <br>
             IEEE International Conference on
Robotics and Automation (ICRA) 2024 Workshop on Back to the Future: Robot Learning Going Probabilistic.
             <br>
             <a href="https://sites.google.com/view/konwloop/home">webiste</a>
             /   
              <a href="https://drive.google.com/file/d/1F2QZ2kfV2EOUCtwg3rzNU_kQyfMN9jEu/view?usp=sharing">poster</a>
              /
              <a href="https://arxiv.org/abs/2406.00430">arxiv</a>
              / 
                <a href="data/icra_ws_2024.bib">bibtex</a>
                <p></p>
                <p>
                An investigation of three different ways for uncertainty quantification of a VLM failure detector for closed-loop LLM planners.
                </p>
        </tr>
     <! ----------------------------------------------------------------------------------------------------------------------------- -->	

        <! ------------------------------------------------------- RASP ----------------------------------------------------------------- -->
        <tr >
           <td style="padding:20px;width:25%;vertical-align:middle">
             <div class="one">
               <a href="https://arxiv.org/pdf/2303.10135.pdf"><img src='images/GRACE_teaser1.png' width="220" height="200"  style="vertical-align:middle;margin:-15px 10px"></a>
               <!-- <embed src="images/GRACE_teaser.pdf" width="190" /> -->
             </div>
           </td>
           <td style="padding:20px;width:75%;vertical-align:middle">
             <a href="https://ieeexplore.ieee.org/document/10342352">
               <papertitle>Efficient and Feasible Robotic Assembly Sequence Planning via Graph Representation Learning</papertitle>
             </a>
             <br>
             <a href="https://aim-lab.io/author/matan-atad/"> Matan Atad </a>*,
             <strong>Jianxiang Feng*</strong>,
             <a href="https://scholar.google.com.uy/citations?user=NeZae4kAAAAJ&hl=es">Ismael Rodríguez</a>,
             <a href="https://rmc.dlr.de/rm/de/staff/maximilian.durner/">Maximilian Durner</a>,
             <a href="https://rmc.dlr.de/rm/de/staff/rudolph.triebel/">Rudolph Triebel</a>,
             <br>
             IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2023.
             <br>
             <a href="https://github.com/DLR-RM/GRACE">code</a>
             /
             <a href="https://sites.google.com/view/asp-grace/home">website</a> 
             /
             <a href="https://arxiv.org/pdf/2303.10135">arXiv</a>
             /
             <a href="data/iros2023.bib">bibtex</a>
             <p></p>
             <p>
             A holistic graphical approach including a graph representation for product assemblies and a policy architecture, Graph Assembly Processing Network, dubbed GRACE to predict assembly sequences in a step-by-step manner.
             </p>
         </tr>
     <! ----------------------------------------------------------------------------------------------------------------------------- -->	
         
     <! ------------------------------------------------------- NF RASP ----------------------------------------------------------------- -->
             <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <!-- <img src='GRACE_teaser.pdf' width="160" height="2100px" > -->
                    <a href="https://openreview.net/forum?id=Zw9rQMDl8m&referrer=[the%20profile%20of%20Jianxiang%20Feng]"><img src='images/NFs_ASP_teaser.png' width="250" height="150" style="vertical-align:middle;margin:10px 0px"></a>
                    <!-- <embed src="images/NFs_ASP_teaser.pdf" width="190" /> -->
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://openreview.net/forum?id=Zw9rQMDl8m&referrer=[the%20profile%20of%20Jianxiang%20Feng]">
                    <papertitle>Density-based Feasibility Learning with
                      Normalizing Flows for Introspective Robotic
                      Assembly</papertitle>
                  </a>
                  <br>
                  <strong>Jianxiang Feng*</strong>,
                  <a href="https://aim-lab.io/author/matan-atad/"> Matan Atad </a>*,
                  <a href="https://scholar.google.com.uy/citations?user=NeZae4kAAAAJ&hl=es">Ismael Rodríguez</a>,
                  <a href="https://rmc.dlr.de/rm/de/staff/maximilian.durner/">Maximilian Durner</a>,
	          <a href="https://www.cs.cit.tum.de/en/daml/team/damlguennemann/">Stephan Günnemann</a>,
                  <a href="https://rmc.dlr.de/rm/de/staff/rudolph.triebel/">Rudolph Triebel</a>,
                  <br>
                  Workshop on Robotics and AI: The Future of Industrial Assembly Tasks,
                  Robotics: Science and Systems (RSS) 2023.
                  <br>
                  <a href="https://github.com/DLR-RM/GRACE">code</a>
                  /
                  <a href="https://sites.google.com/view/nfasp/home">website</a> 
                  /
                  <a href="https://arxiv.org/abs/2307.01317">arxiv</a>
                  /
                  <a href="data/rss_ws_nf4rasp_v2.mp4">presentation video</a>
                  /
                  <a href="data/rss_ws_nf4rasp_v2.pdf">slides</a>
                  /
                  <a href="data/rss_ws_2023.bib">bibtex</a>
                  <p></p>
                  <p>
                    A density-based method with normalizing flows for feasibility learning in Robotic Assembly based on only feasible examples.
                  </p>
              </tr>
      <! ----------------------------------------------------------------------------------------------------------------------------- -->	


  <! ------------------------------------------------------- Survey ----------------------------------------------------------------- -->
  <!--     <tr onmouseout="bal_edan_stop()" onmouseover="bal_edan_start()"  bgcolor="#ffffd0"> -->
     <tr >
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <a href="https://arxiv.org/abs/2107.03342"><img src='images/survey.png' width="245" height="160"></a>
            <!-- <embed src="images/survey1.pdf" width="190" /> -->
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://link.springer.com/article/10.1007/s10462-023-10562-9">
            <papertitle>A Survey of Uncertainty in Deep Neural Networks </papertitle>
          </a>
          <br>
          <a href="https://scholar.google.com/citations?user=JSDN9rsAAAAJ&hl=en">Jakob Gawlikowski</a>,
          Cedrique Rovile Njieutcheu Tassi, 
          Mohsin Ali, 
          <a href="https://rmc.dlr.de/rm/en/staff/jongseok.lee/">Jongseok Lee</a>,
          <a href="https://www.hummat.com/">Matthias Humt</a>,
          <strong>Jianxiang Feng</strong>,
          Anna Kruspe, 
          <a href="https://rmc.dlr.de/rm/de/staff/rudolph.triebel/">Rudolph Triebel</a>,
          Peter Jung, 
          Ribana Roscher, 
          Muhammad Shahzad, 
          Wen Yang, 
          Richard Bamler, 
          <a href="https://www.professoren.tum.de/en/zhu-xiaoxiang"> Xiaoxiang Zhu</a>
          <br> 
		Artificial Intelligence Review (2023): 1-77
          <br>
          	<a href="https://arxiv.org/abs/2107.03342">arXiv</a>
	        /
          <a href="data/survey.bib">bibtex</a> 
          <p></p>
          <p>
           A comprehensive overview of uncertainty estimation in neural networks, reviewing recent advances in the field, highlighting current challenges, and identifying potential research opportunities. 
          </p>
      </tr>
  <! ----------------------------------------------------------------------------------------------------------------------------- -->	
	
<! ------------------------------------------------------- SAM ----------------------------------------------------------------- -->
<!--    <tr onmouseout="FR_SAM_stop()" onmouseover="FR_SAM_start()"  bgcolor="#ffffd0"> -->
  <tr >
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <!-- <div class="two" id='FR_SAM_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/bal_edan.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div> -->
          <a href="https://elib.dlr.de/194517/1/Vol3_10.pdf"><img src='images/FR2023.png' width="245"  style="vertical-align:middle;margin:0px 5px"></a>
        </div>
      </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://elib.dlr.de/194517/1/Vol3_10.pdf">
        <papertitle>Virtual Reality via Object Pose Estimation and Active Learning: Realizing Telepresence Robots with Aerial Manipulation Capabilities</papertitle>
      </a>
      <br>
      <a href="https://rmc.dlr.de/rm/en/staff/jongseok.lee/">Jongseok Lee</a>, <a href="https://rmc.dlr.de/rm/en/staff/ribin.balachandran/">Ribin Radhakrishna Balachandran</a> , <a href="https://de.linkedin.com/in/konstantin-kondak-02265526">Konstantin Kondak</a>, <a href="https://scholar.google.com.br/citations?user=HZlxXRYAAAAJ&hl=en">Andre Coelho</a>, <a href="https://rmc.dlr.de/rm/de/staff/marco.destefano/">Marco De Stefano</a>, <a href="https://www.hummat.com/">Matthias Humt</a>, <strong>Jianxiang Feng</strong>, <a href="https://h2t.anthropomatik.kit.edu/21_2372.php">Tamim Asfour</a>,   <a href="https://rmc.dlr.de/rm/de/staff/rudolph.triebel/">Rudolph Triebel</a>,
    
      <br>
      Field Robotics, 2023
      <br>
      <a href="https://www.youtube.com/watch?v=JRnPIARW8xY&ab_channel=DLRRM">video</a>
      /
      <a href="data/fr2023.bib">bibtex</a>
      <!-- /
      <a href="data/bal2022.bib">bibtex</a> -->
      <p></p>
      <p>
        A novel teleoperation system for advancing aerial manipulation in dynamic and unstructured environments based on pose estimation pipelines for the industrial objects of
        both known and unknown geometries and an active learning pipeline.
      </p>
    </td>
  </tr>
  <! ------------------------------------------------------------------------------------------------------------------------ -->	
	
<! ------------------------------------------------------- ICML2020 ----------------------------------------------------------------- -->
<!--     <tr onmouseout="bal_edan_stop()" onmouseover="bal_edan_start()"  bgcolor="#ffffd0"> -->
   <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
            <!-- <embed src="images/icml20.pdf" width="190" /> -->
            <a href="http://proceedings.mlr.press/v119/lee20b.html"><img src='images/icml20.png' width="205" height="200"  style="vertical-align:middle;margin:0px 25px"></a>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="http://proceedings.mlr.press/v119/lee20b.html">
          <papertitle>Estimating Model Uncertainty of Neural Networks in Sparse Information Form  </papertitle>
        </a>
        <br>
        <a href="https://rmc.dlr.de/rm/en/staff/jongseok.lee/">Jongseok Lee</a>,
        <a href="https://www.hummat.com/">Matthias Humt</a>,
        <strong>Jianxiang Feng</strong>,
        <a href="https://rmc.dlr.de/rm/de/staff/rudolph.triebel/">Rudolph Triebel</a>,
        <br>
        International Conference on Machine Learning (ICML) 2020.
        <br>
        <a href="https://github.com/DLR-RM/curvature">code</a>
        /
        <!-- <a href="https://www.youtube.com/watch?v=vu2TnDEqDRk&t=37s&ab_channel=DLRRM">video</a>
        / -->
        <!-- <a href="https://arxiv.org/pdf/2109.12869">arXiv</a>
        / -->
        <a href="data/pmlr-v119-lee20b.bib">bibtex</a>
        <p></p>
        <p>
         A sparse representation of model uncertainty for Deep Neural Networks (DNNs) where the parameter posterior is approximated with an inverse formulation of the Multivariate Normal Distribution (MND), also known as the information form. 
        </p>
    </tr>
<! ----------------------------------------------------------------------------------------------------------------------------- -->	
	

<! ------------------------------------------------------- End of Other Publications ----------------------------------------------------------------- -->
		
<! ------------------------------------------------------- Pre-prints ----------------------------------------------------------------- -->
	        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
	          <tbody>
	            <tr>
	            <td style="padding:20px;width:100%;vertical-align:middle">
                <br>
                <br>
                <h1 style="font-size: 30px">Preprints</h1>
                <br>
	            </td>
	          </tr>
	        </tbody>
	      </table>
	      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <! ------------------------------------------------------- FFHFlow ----------------------------------------------------------------- -->
          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <a href="https://arxiv.org/abs/2407.15161"><img src='images/ffhflow.png' width="240" height="200"  style="vertical-align:middle;margin:-20px 0px"></a>
                <!-- <embed src="images/GRACE_teaser.pdf" width="190" /> -->
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2407.15161">
                <papertitle>FFHFlow: A Flow-based Variational Approach for Multi-fingered Grasp Synthesis in Real Time</papertitle>
              </a>
              <br>
                    <strong>Jianxiang Feng</strong>*,
                    <a href="https://www.ce.cit.tum.de/air/people/qian-feng-msc/">Qian Feng</a>*,
                    <a href="https://dblp.org/pid/63/8729.html">Zhaopeng Chen</a>,
                    <a href="https://rmc.dlr.de/rm/de/staff/rudolph.triebel/">Rudolph Triebel</a>,
                    <a href="https://www.ce.cit.tum.de/air/people/prof-dr-ing-habil-alois-knoll/"> Alois Knoll</a>,
              <br>
              <br>
              <a href="https://arxiv.org/abs/2407.15161">arxiv</a> /
              <a href="https://sites.google.com/view/ffhflow/home/">website</a>
                  <p></p>
                  <p>
                    A novel normalizing flows-based variational approach to tackle the latent feature collapse issue in VAE-based approaches for diverse and efficient Grasp Synthesis.
                  </p>
          </tr>

          <! ------------------------------------------------------- End of FFHFlow ----------------------------------------------------------------- -->

        <! ------------------------------------------------------- Dynamic Grasping ----------------------------------------------------------------- -->
              <tr >
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <a href="https://arxiv.org/abs/2310.17923"><img src='images/dynamic_grasping.png' width="240" height="235"  style="vertical-align:middle;margin:0px 0px"></a>
                    <!-- <embed src="images/GRACE_teaser.pdf" width="190" /> -->
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2310.17923">
                    <papertitle>Multi-fingered Dynamic Grasping for Unknown Objects</papertitle>
                  </a>
                  <br>
                        <a href="https://srl.cit.tum.de/members/burkhary">Yannick Burkhardt</a>*,
                        <a href="https://www.ce.cit.tum.de/air/people/qian-feng-msc/">Qian Feng</a>*,
                        <strong>Jianxiang Feng</strong>,
                        <a href="https://scholar.google.de/citations?user=_dL0__MAAAAJ&hl=en">Karan Sharma</a>,
                        <a href="https://dblp.org/pid/63/8729.html">Zhaopeng Chen</a>,
                        <a href="https://www.ce.cit.tum.de/air/people/prof-dr-ing-habil-alois-knoll/"> Alois Knoll</a>,
                  <br>
                  <br>
                  <a href="https://arxiv.org/abs/2310.17923">arxiv</a> /
                  <a href="https://youtu.be/b87zGNoKELg">video</a>
                      <p></p>
                      <p>
                      A dynamic grasping framework for unknown objects in this work, which uses a five-fingered hand with visual servo control and can compensate for external disturbances..
                      </p>
              </tr>
              <! ------------------------------------------------------- End of Dynamic Grasping ----------------------------------------------------------------- -->

<! ------------------------------------------------------- End of Pre-prints ----------------------------------------------------------------- -->

				
	<! ------------------------------------------------------- Academic Services ---------------------------------------------------------- -->	
  
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <br>
              <br>
              <br>
              <h1 style="font-size: 30px">Academic Services</h1>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">

              Conference/Journal Reviewer: 
              <ul>
                <li>Conference on Robot Learning (CoRL) 2022-2024</li>
                <li>IEEE International Conference on Robotics and Automation (ICRA) 2020&2022</li>
                <li>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2020&2022</li>
                <li>European Conference on Artificial Intelligence (ECAI) 2020</li>
                <li>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</li>
              </ul>  
              <br>
              <br>
              Workshop Organizer: 
              <ul>
                <li>ICRA2024 Workshop on <a href="https://probabilisticrobotics.github.io/">Back to the Future: Robot Learning Going Probabilistic</a></li> 
                <li>IROS2022 Workshop on <a href="https://probabilisticrobotics.github.io/2022/">Probabilistic Robotics in the age of Deep Learning</a></li> 
              </ul>  
              <br>

        <!-- <div class="list-item highlight" data-category="highlight">
                <p class="date">2024</p> &nbsp;&nbsp;&nbsp; Reviewer, CoRL.
              </div>	    

        <div class="list-item highlight" data-category="highlight">
                <p class="date">05.2024</p> &nbsp;&nbsp;&nbsp; Co-Organizer, ICRA Workshop on <a href="https://probabilisticrobotics.github.io/">Back to the Future: Robot Learning Going Probabilistic</a>.
              </div>
          
        <div class="list-item highlight" data-category="highlight">
                <p class="date">2023</p> &nbsp;&nbsp;&nbsp; Reviewer, CoRL.
              </div>	    

              <div class="list-item highlight" data-category="highlight">
              <p class="date">10.2022</p> &nbsp;&nbsp;&nbsp; Co-Organizer, IROS Workshop on <a href="https://probabilisticrobotics.github.io/2022/">Probabilistic Robotics in the age of Deep Learning</a>.
              </div>
                
              <div class="list-item highlight" data-category="highlight">
                <p class="date">2022</p> &nbsp;&nbsp;&nbsp; Reviewer, ICRA, CoRL, IROS.
              </div>
            
              <div class="list-item highlight" data-category="highlight">
                <p class="date">2020</p> &nbsp;&nbsp;&nbsp; Reviewer, ICRA, IROS, ECAI.
              </div> -->
              
            
            </td>
          <!-- </tr> -->
      
	<! ------------------------------------------------------------------------------------------------------------------------ -->		
				
	<! ------------------------------------------------------- Events ---------------------------------------------------------- -->	
  
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <h1 style="font-size: 30px">Events</h1>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">

		    
	   <div class="list-item highlight" data-category="highlight">
		<p class="date">06.2023</p> &nbsp;&nbsp;&nbsp; Team player, the <a href=https://www.linkedin.com/feed/update/urn:li:activity:7091522148384550913/?origin=SHARED_BY_YOUR_NETWORK>aerial manipulation demo</a> for <a href="https://www.kuka.com/en-us/future-production/research-and-development/kuka-innovation-award/kuka-innovation-award-2023">kuka inovation award 2023</a> in <a href="https://automatica-munich.com/en/">Automatica</a> 2023 (<a href="https://www.linkedin.com/posts/kukaglobal_team-jarvis-from-the-merlin-labodratory-of-activity-7080170709481574400-7guO?utm_source=share&utm_medium=member_desktop">press</a>).
	   </div><div class="list-item highlight" data-category="highlight">
			
	    <div class="list-item highlight" data-category="highlight">
		 <p class="date">03.2023</p> &nbsp;&nbsp;&nbsp; Team player, the 1st place in the discipline assist robot race of <a href="https://cybathlon.ethz.ch/de/teams/edan">Cybathlon Challenge</a> as part of EDAN team (<a href="https://www.dlr.de/de/rm/aktuelles/nachrichten/2023/teilnahme-an-den-cybathlon-challenges-2023">press</a>, <a href="https://www.youtube.com/watch?v=EoER_5vYZsU&t=3s&ab_channel=DLRRM">video</a>)
            </div>
			
          <div class="list-item highlight" data-category="highlight">
              <p class="date">06.2022</p> &nbsp;&nbsp;&nbsp; Team player, <a href="https://www.dlr.de/rm/desktopdefault.aspx/tabid-11670/#gallery/28208">EDAN</a> demo at Automatica Exhibition 2022 (<a href="https://www.dlr.de/de/rm/aktuelles/nachrichten/2022/automatica-2022">press</a>).
            </div>
            
            </td>
          <!-- </tr> -->
      
	<! ------------------------------------------------------------------------------------------------------------------------ -->		
				
	<! ------------------------------------------------------- Misc ---------------------------------------------------------- -->	


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <h1 style="font-size: 30px">Mentorship</h1>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
		    
	<div class="list-item highlight" data-category="highlight">
            <p class="date">08.2023</p> &nbsp;&nbsp;&nbsp; Master Thesis Supervision at TUM: "Improving Sample Selection in Active Learning Using Graph Neural Networks" by Zhoumin Zhao, co-supervision with <a href="https://www.cs.cit.tum.de/en/daml/team/simon-geisler/">Simon Geisler</a>.
           </div>  
           <br>

		    
	  <div class="list-item highlight" data-category="highlight">
            <p class="date">06.2023</p> &nbsp;&nbsp;&nbsp; Research Internship at DLR: "Automating Scene Graph Data Generation for Task and Motion Planning via Blenderproc" by Juan Diego Plaza Gomez, co-supervision with <a href="https://rmc.dlr.de/rm/de/staff/samuel.bustamante/">Samuel Bustamante</a> and <a href="https://scholar.google.de/citations?user=kduGd8wAAAAJ&hl=de">Dominik Winkelbauer</a>.
           </div>  
           <br>
		    
		
            <div class="list-item highlight" data-category="highlight">
            <p class="date">03.2023</p> &nbsp;&nbsp;&nbsp; Master Thesis Supervision at TUM: <a href="https://elib.dlr.de/194212/">"Graph Neural Networks for Knowledge Transfer in Robotic Assembly Sequence Planning"</a> by Matan Atad, co-supervision with <a href="https://rmc.dlr.de/rm/de/staff/maximilian.durner/">Maximilian Durner</a>, <a href="https://rmc.dlr.de/rm/en/staff/ismael.rodriguezbrena/index.html">Ismael Rodriguezbrena</a>.
           </div>  
           <br>
          
            <div class="list-item highlight" data-category="highlight">
              <p class="date">11.2022</p> &nbsp;&nbsp;&nbsp; Master Thesis Supervision at TUM: <a href="https://elib.dlr.de/194212/">"Scene Graph Generation from Visual perception for Task and Motion Planning"</a> by Mohit Kumar, co-supervision with <a href="https://rmc.dlr.de/rm/en/staff/samuel.bustamante/">Samuel Bustamante</a>.
            </div>  
            <br>
            
            </td>
          <!-- </tr> -->
		  
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
        <tr>
          <td>
            <h1 style="font-size: 30px">Fun Moments and Hobbies</h1>
          </td>
        </tr>
      </tbody></table>
      <table width="100%" align="center" border="0" cellpadding="20"><tbody>
        <tr>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="https://www.instagram.com/luffy_fjx/">Instagram</a>
      
	<! ------------------------------------------------------------------------------------------------------------------------ -->				
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://github.com/jonbarron/jonbarron_website">Link to the template</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

  <script>
        function toggle_bio() {
        var x = document.getElementById("more-bio");
        if (x.style.display === "none") {
          x.style.display = "block";
        } else {
          x.style.display = "none";
        }
      }
  </script>
</body>

</html>
